[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rafael Sant’Ana Herzog",
    "section": "",
    "text": "twitter\n  \n  \n    \n     Github\n  \n\n  \n  \nOlá! Me chamo Rafael Sant’Ana Herzog, e você está em meu humilde site - que ainda está claramente em construção.\nSou um futuro estatístico que considera seriamente fazer mestrado em computação e que gosta de passar seu tempo livre assistindo séries, filmes e jogando alguns dos poucos jogos que ainda consegue.\n\n\n\n\nEstudante do 8º período do bacharelado em Estatística (com formatura prevista para agosto de 2024)\n\nMarço de 2020 até o presente\nCoeficiente de Rendimento de 9,49\n\n\n\n\n\nDesde dezembro de 2021, sou bolsista no Observatório Obstétrico Brasileiro (OOBr), onde analiso bases de dados públicos, escrevo publicações relacionadas a temas dentro da Estatística e construo e atualizo painéis de visualização utilizando o pacote {shiny}, do software R."
  },
  {
    "objectID": "index.html#educação",
    "href": "index.html#educação",
    "title": "Rafael Sant’Ana Herzog",
    "section": "",
    "text": "Estudante do 8º período do bacharelado em Estatística (com formatura prevista para agosto de 2024)\n\nMarço de 2020 até o presente\nCoeficiente de Rendimento de 9,49"
  },
  {
    "objectID": "index.html#experiência",
    "href": "index.html#experiência",
    "title": "Rafael Sant’Ana Herzog",
    "section": "",
    "text": "Desde dezembro de 2021, sou bolsista no Observatório Obstétrico Brasileiro (OOBr), onde analiso bases de dados públicos, escrevo publicações relacionadas a temas dentro da Estatística e construo e atualizo painéis de visualização utilizando o pacote {shiny}, do software R."
  },
  {
    "objectID": "paineis.html",
    "href": "paineis.html",
    "title": "Painéis de visualização",
    "section": "",
    "text": "John Doe\n\n\nSome example text.\n\nSee Profile\n\n\n\n\n\nOne\n\n\nTwo\n\n\nThree\n\n\nFour\n\n\nFive\n\n\n\n\n\n\n\n\nJohn Doe\n\n\nSome example text.\n\nSee Profile\n\n\n\nTwo\n\nI have some more content in.\n\n\nThis makes me taller than 100 pixels.\n\n\n\n\n\n\n\nJohn Doe\n\n\nSome example text.\n\nSee Profile\n\n\n\nFour\n\n\nFive\n\n\n\n\n\n\n\n\nJohn Doe\n\n\nSome example text.\n\nSee Profile\n\n\n\n\nTwo\n\n\nThree\n\n\n\n\n\n\nFive"
  },
  {
    "objectID": "posts/15-03-2022-funcao-de-verossimilhanca/index.html",
    "href": "posts/15-03-2022-funcao-de-verossimilhanca/index.html",
    "title": "Função de Verossimilhança",
    "section": "",
    "text": "Introdução\nNeste post, iremos conhecer sobre o conceito de função de verossimilhança, um tema de extrema importância dentro inferência estatística. Discutiremos sobre algumas de suas utilizações e realizaremos exemplos práticos e computacionais que nos permitam absorver melhor as ideias que aqui forem abordadas. Para começo de conversa, quando estudamos a Teoria das Probabilidades, nos habituamos a ter um conhecimento prévio sobre a distribuição de probabilidade que uma determinada variável aleatória segue, podendo, assim, obter respostas para os possíveis eventos que podem vir a acontecer em relação a essa variável. Em situações cotidianas, entretanto, pode ser extremamente difícil de se obter precisamente as distribuições de probabilidade de certas variáveis aleatórias. Na maior parte dos casos, o melhor que podemos fazer é obter uma amostra da população que queremos estudar, e, a partir dessa amostra, inferenciar sobre a população em questão. Temos aqui uma mudança de perspectiva: se antes estávamos interessados em calcular a probabilidade de uma variável aleatória assumir um certo valor, dado que ela segue uma certa distribuição de probabilidade, agora queremos estudar sobre como deve ser essa distribuição de probabilidade, dado que observamos certos valores dessa variável aleatória na amostra.\nAssim, comecemos por uma definição: considere \\(f_{\\mathbf{X}}(\\mathbf{x}|\\theta)\\) como sendo a função densidade conjunta (no caso contínuo) ou a função de probabilidade conjunta (no caso discreto) de \\(n\\) variáveis aleatórias. Definimos a função de verossimilhança \\(L(\\theta|\\mathbf{x})\\) como sendo a função densidade (ou de probabilidade) conjunta dessas \\(n\\) variáveis aleatórias vista como função de \\(\\theta\\), ou seja, considerando como fixos os valores de \\(X_1,\\;X_2,\\;...,\\;X_n\\) e variando o valor do parâmetro \\(\\theta\\) (ou valores, caso \\(\\theta\\) seja um vetor de parâmetros). Em outras palavras, temos que\n\\[\nL(\\theta|\\mathbf{x}) = f_{\\mathbf{X}}(\\mathbf{x}|\\theta)\n\\]\nApesar de, à primeira vista, a definição acima parecer ser simplesmente uma diferença de notação, a função de verossimilhança e a função densidade (ou de probabilidade) conjunta apresentam distinções importantíssimas. Em primeiro lugar, algo a se notar é que a função de verossimilhança, por ser uma função de \\(\\theta\\), pode ser contínua em um certo intervalo mesmo que \\(f_{\\mathbf{X}}(\\mathbf{x}|\\theta)\\) seja uma função de probabilidade conjunta (ou seja, mesmo que as variáveis aleatórias que fazem parte da amostra sejam discretas). Basta que \\(\\theta\\) assuma valores de forma contínua em um dado intervalo. Além disso, é preciso deixar claro que a função de verossimilhança não é uma função densidade (ou de probabilidade) em relação a \\(\\theta\\), pois sua integral (ou soma) em todo o espaço paramétrico (conjunto de todos os possíveis valores que o parâmetro pode assumir) não necessariamente é igual a 1, propriedade que deve ser cumprida para que uma função seja considerada uma função densidade de probabilidade ou uma função de probabilidade. Dessa forma, o valor obtido através da função de verossimilhança não pode ser considerado como uma densidade de probabilidade ou uma probabilidade em relação a \\(\\theta\\). De fato, chamamos, pásmem, de verossimilhança o valor obtido por meio dessa função. Mas, afinal… o que seria, então, essa verossimilhança?\n\n\nEntendendo a verossimilhança\nPara explicar o que é, de fato, a verossimilhança, precisamos definir dois cenários diferentes. Para o primeiro caso, o caso discreto, considere uma amostra aleatória \\((X_1,\\;X_2,\\;X_3,\\;X_4)\\) na qual todas as variáveis seguem uma distribuição \\(Bernoulli(\\theta)\\), onde \\(\\theta\\) é a probabilidade de sucesso, variando no intervalo \\([0,\\;1]\\). Suponha que os valores observados de \\((X_1,\\;X_2,\\;X_3,\\;X_4)\\) tenham sido \\(x_1 = 0,\\;x_2 = 1,\\;x_3 = 1\\) e \\(x_4 = 1\\), e que queremos decidir entre duas hipóteses: a hipótese \\(A\\), na qual o verdadeiro valor do parâmetro seria \\(\\theta = 0,5\\), e a hipótese \\(B\\), na qual o valor de \\(\\theta\\) seria 0,8. Por se tratar de uma amostra aleatória, consideramos que todas as variáveis são independentes e identicamente distribuídas (i.i.d.). Dessa forma, a função de probabilidade conjunta pode ser escrita como sendo o produtório das funções de probabilidade marginais (é de extrema importância lembrar que essa afirmação é válida se, e somente se, as variáveis aleatórias em questão são independentes). Assim, para \\(x \\in \\{0,\\;1\\}\\), temos:\n\\[\n\\begin{align}\nf(\\mathbf{x}|\\theta) &= \\prod_{i = 1}^4 \\theta^{x_i} (1 - \\theta)^{1 - x_i} \\\\\n                     &= \\theta^{\\sum_{i = 1}^4 x_i}(1 - \\theta)^{4 - \\sum_{i = 1}^4 x_i}\n\\end{align}\n\\]\nSendo \\(\\sum_{i = 1}^4 x_i = 0 + 1 + 1 + 1 = 3\\),\n\\[\n\\begin{align}\nf(\\mathbf{x}|\\theta) &= \\theta^{3}(1 - \\theta)^{4 - 3} \\\\\n                     &= \\theta^{3}(1 - \\theta)\n\\end{align}\n\\]\nAssim, \\(L(\\theta|\\mathbf{x}) = \\theta^{3}(1 - \\theta)\\), para \\(0 \\leqslant \\theta \\leqslant 1\\). Note que, como \\(f_{\\mathbf{X}}(\\mathbf{x}|\\theta)\\) é uma função de probabilidade conjunta e, portanto, está limitada ao intervalo \\([0,\\;1]\\), a função de verossimilhança também estará limitada a esse intervalo. Antes de continuarmos a resolução do exemplo, podemos ilustrar que, como dito na seção anterior, a integral da função de verossimilhança em todo o seu domínio não necessariamente deve ser igual a 1. Observe os cálculos abaixo.\n\\[\n\\begin{align}\n\\int_0^1 \\theta^3(1 - \\theta) d\\theta = \\int_0^1 \\theta^3 - \\theta^4 = \\left(\\frac{\\theta^4}{4} - \\frac{\\theta^5}{5}\\right) \\Bigg\\rvert_0^1 = \\frac{1}{4} - \\frac{1}{5} = \\frac{1}{20}\n\\end{align}\n\\]\nDessa forma, reforçando, \\(L(\\theta|\\mathbf{x})\\) não pode ser considerada uma função densidade de probabilidade em relação a \\(\\theta\\). Voltando ao exemplo, queríamos decidir entre as hipóteses \\(A\\), na qual \\(\\theta = 0,5\\), e \\(B\\), na qual \\(\\theta = 0,8\\). No primeiro caso, o valor da verossimilhança seria dado por:\n\\[\n\\begin{align}\nL(0,5|\\mathbf{x}) & = 0,5^3(1 - 0,5) \\\\\n                  & = 0,125(0,5) \\\\\n                  & = 0,0625\n\\end{align}\n\\]\nJá no segundo caso,\n\\[\n\\begin{align}\nL(0,8|\\mathbf{x}) & = 0,8^3(1 - 0,8) \\\\\n                  & = 0,512(0,2) \\\\\n                  & = 0,1024\n\\end{align}\n\\]\nQuanto à interpretação dos resultados, não podemos dizer, de forma alguma, que a verossimilhança indica que a probabilidade do verdadeiro valor de \\(\\theta\\) ser 0,5 é de 0,0625, ou que a probabilidade de \\(\\theta\\) ser 0,8 é de 0,1024. O valor da verossimilhança, quando \\(\\mathbf{X}\\) é discreto, deve ser interpretado como sendo a probabilidade de observarmos os valores da amostra caso o verdadeiro valor do parâmetro seja o valor testado. Assim, a verossimilhança seria uma probabilidade em relação a X, e não em relação a \\(\\theta\\). Com isso, como 0,1024 &gt; 0,0625, concluímos que a amostra em questão tem uma maior chance de ser observada quando \\(\\theta = 0,8\\), e, portanto, a hipótese \\(B\\) é mais verossímil do que a hipótese \\(A\\). Podemos ainda definir a razão de verossimilhança, dada por \\[\n\\frac{L(B|\\mathbf{x})}{L(A|\\mathbf{x})},\n\\]\ncomo sendo uma forma de medir a força de evidência em favor da hipótese \\(B\\) sobre a hipótese \\(A\\). Nesse exemplo, temos que\n\\[\n\\frac{L(B|\\mathbf{x})}{L(A|\\mathbf{x})} = \\frac{0,1024}{0,0625} = 1,6384.\n\\] Assim, podemos dizer que a observação da amostra obtida é evidência de que a hipótese \\(B\\) é aproximadamente 1,64 vezes mais verossímil do que a hipótese \\(A\\).\nAnalogamente, caso \\((X_1,\\;X_2,\\;...,\\;X_n)\\) fosse uma amostra de variáveis aleatórias que seguem uma distribuição contínua, teríamos que \\(f_\\mathbf{X}(\\mathbf{x}|\\theta)\\) seria uma função densidade conjunta, e que, por estar definida em \\([0, \\infty)\\), assim o estaria a função de verossimilhança. O valor da verossimilhança, nesse cenário, indicaria a densidade de probabilidade da amostra observada caso o verdadeiro valor de \\(\\theta\\) fosse o valor testado. Novamente, a densidade de probabilidade é em relação a X, e não em relação a \\(\\theta\\). De qualquer forma, encontrar o valor de \\(\\theta\\) que maximiza uma função de verossimilhança aparenta ser um bom caminho para estimar o verdadeiro valor do parâmetro de uma distribuição de probabilidade. E é exatamente sobre isso que discutiremos na próxima seção.\n\n\nEstimação de parâmetros pelo método da máxima verossimilhança\nNo ramo da inferência estatística, um dos métodos mais conhecidos e dominantes de se estimar um parâmetro de uma certa distribuição de probabilidade, dado os valores observados em uma amostra, é o chamado método da máxima verossimilhança. A ideia principal desse método consiste em maximizar o valor da função de verossimilhança, para que assim a probabilidade de a amostra observada ocorrer seja a maior possível. Dessa forma, sendo \\(L(\\theta|\\mathbf{x})\\) a função de verossimilhança das variáveis aleatórias \\(X_1,\\;X_2,\\;...,\\;X_n\\), chamamos de estimador de máxima verossimilhança e denotamos por \\(\\hat{\\theta}\\), sendo \\(\\hat{\\theta}(X_1,\\;X_2,\\;...,\\;X_n)\\) (ou seja, uma função das variáveis aleatórias), o valor de \\(\\theta\\) que, entre todo o espaço paramétrico, maximiza \\(L(\\theta|\\mathbf{x})\\). Quando substituídos os valores observados de \\(X_1,\\;X_2,\\;...,\\;X_n\\), obtemos a chamada estimativa de máxima verossimilhança para \\(\\theta\\).\nPara maximizar a função de verossimilhança, buscaremos auxílio no Cálculo. Respeitadas algumas condições, temos que o estimador de máxima verossimilhança é a solução da equação\n\\[\n\\frac{dL(\\theta|\\mathbf{x})}{d\\theta} = 0.\n\\] Para que as explicações acima fiquem mais claras, passemos para um exemplo. Considere que \\((X_1,\\;X_2,\\;...,\\,X_n)\\) é uma amostra aleatória de uma distribuição \\(Bernoulli(\\theta)\\). De forma semelhante ao que vimos na seção anterior, podemos escrever a função de probabilidade conjunta dessas variáveis, para \\(x \\in \\{0,\\;1\\}\\) como sendo\n\\[\n\\begin{align}\nf_\\mathbf{X}(\\mathbf{x}|\\theta) &= \\prod_{i = 1}^n \\theta^{x_i}(1 - \\theta)^{1 - x_i} \\\\\n                                &= \\theta^{\\sum_{i = 1}^n x_i}(1 - \\theta)^{n - \\sum_{i = 1}^n x_i}\n\\end{align}\n\\] Logo, \\(L(\\theta|\\mathbf{x}) = \\theta^{\\sum_{i = 1}^n x_i}(1 - \\theta)^{n - \\sum_{i = 1}^n x_i}\\) para \\(0 \\leqslant \\theta \\leqslant 1\\). Para encontrarmos o estimador de máxima verossimilhança de \\(\\theta\\), precisamos derivar a função \\(L(\\theta|\\mathbf{x})\\) em relação a \\(\\theta\\) e igualar o resultado a zero. Note, entretanto, que na maioria dos casos é consideravelmente mais fácil encontrar a derivada do logaritmo da função de verossimilhança do que a derivada da função de verossimilhança propriamente dita. E como a função logarítmica é estritamente crescente, maximizar a função de verossimilhança é o equivalente a maximizar a função de log-verossimilhança. Para não causar maiores confusões, denotaremos essa última por \\(l(\\theta|\\mathbf{x})\\), sendo \\(l(\\theta|\\mathbf{x}) = \\log\\left[L(\\theta|\\mathbf{x})\\right]\\). Assim, temos:\n\\[\n\\begin{align}\nl(\\theta|\\mathbf{x}) &= \\log\\left[\\theta^{\\sum_{i = 1}^n x_i}(1 - \\theta)^{n - \\sum_{i = 1}^n x_i}\\right] \\\\\n                     &= \\log\\left(\\theta^{\\sum_{i = 1}^n x_i}\\right) + \\log\\left[(1 - \\theta)^{n - \\sum_{i = 1}^n x_i}\\right] \\\\\n                     &= \\sum_{i = 1}^n x_i\\log(\\theta) + \\left(n - \\sum_{i = 1}^n x_i\\right)\\log(1 - \\theta)\n\\end{align}\n\\] Calculando a derivada em relação a \\(\\theta\\),\n\\[\n\\begin{align}\n\\frac{d\\;l(\\theta|\\mathbf{x})}{d\\theta} &= \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n - \\sum_{i = 1}^n x_i}{1 - \\theta} \\\\\n                                        &= \\frac{(1 - \\theta)\\sum_{i = 1}^n x_i - \\theta\\left(n - \\sum_{i = 1}^n x_i\\right)}{\\theta(1 - \\theta)} \\\\\n                                        &= \\frac{\\sum_{i = 1}^n x_i - \\theta\\sum_{i = 1}^n x_i - n\\theta + \\theta\\sum_{i = 1}^n x_i}{\\theta(1 - \\theta)} \\\\\n                                        &= \\frac{\\sum_{i = 1}^n x_i - n\\theta}{\\theta(1 - \\theta)}\n\\end{align}\n\\] O resultado acima - a derivada da função de log-verossimilhança - recebe um nome especial: a chamamos de função escore, denotada por \\(s(\\theta)\\). Igualando-a a zero, temos:\n\\[\n\\begin{align}\n\\frac{\\sum_{i = 1}^n x_i - n\\theta}{\\theta(1 - \\theta)} = 0 \\quad & \\rightarrow\\quad \\sum_{i = 1}^n x_i - n\\theta = 0 \\\\\n                                                           & \\rightarrow\\quad \\hat{\\theta} = \\frac{\\sum_{i = 1}^n x_i}{n}\n\\end{align}\n\\]\nLogo, o estimador de máxima verossimilhança para \\(\\theta\\) de uma amostra aleatória com distribuição \\(Bernoulli(\\theta)\\) é \\(\\hat{\\theta} = \\bar{X}\\), ou seja, a média amostral. Aplicando esse resultado no exemplo discutido na seção anterior, no qual \\(n = 4\\) e \\(x_1 = 0,\\;x_2 = 1,\\;x_3 = 1\\) e \\(x_4 = 1\\), temos:\n\\[\n\\begin{align}\n\\hat{\\theta} = \\frac{\\sum_{i = 1}^4 x_i}{4} = \\frac{0 + 1 + 1 + 1}{4} = \\frac{3}{4} = 0,75\n\\end{align}\n\\] Assim, a estimativa de máxima verossimilhança para \\(\\theta\\) é de 0,75. Ou seja, a maior probabilidade de observarmos a amostra em questão ocorre quando o valor do parâmetro populacional é 0,75. Nesse ponto, a verossimilhança é dada por\n\\[\n\\begin{align}\nL(0,75|\\mathbf{x}) & = (0,75)^3(1 - 0,75) \\\\\n                   & = 0,421875(0,25) \\\\\n                   & = 0,10546875\n\\end{align}\n\\] Note que, como esperado, o valor acima é maior que o valor encontrado para a verossimilhança quando \\(\\theta = 0,8\\), calculado anteriormente. Assim, podemos considerar que uma hipótese \\(C\\), que diz que o verdadeiro valor do parâmetro é 0,75, é a hipótese mais verossímil para explicar o comportamento de \\(\\theta\\). Muito importante: isso não significa que o verdadeiro valor do parâmetro é, de fato, 0,75. Estamos apenas adotando esse valor como uma estimativa desse parâmetro, uma vez que, pelo método da máxima verossimilhança, ele é o valor que melhor explica a amostra obtida.\nPara concluir, podemos utilizar o R para criar os gráficos das funções de verossimilhança e escore do exemplo acima, para assim termos uma melhor visualização dos resultados obtidos. Definindo as funções, temos:\n\nvero &lt;- function(theta) theta^3*(1-theta)\nescore &lt;- function(theta) (3 - 4*theta)/(theta*(1 - theta))\n\nPara plotarmos os gráficos, utilizaremos a função curve, do pacote básico graphics, a qual desenhará uma curva para a função especificada em um dado intervalo. Para ambas as funções que queremos plotar, como \\(0 \\leqslant \\theta \\leqslant 1\\), não precisaremos especificar o intervalo que curve utilizará, uma vez que \\([0,\\;1]\\) é o intervalo padrão dessa função. Com a função abline, traçaremos linhas verticais e horizontais nas coordenadas especificadas, utilizando o argumento lty para especificar se a linha deve ser completa (lty = 1) ou tracejada (lty = 2). Com a função points, adicionaremos ao gráfico pontos nas coordenadas especificadas, utilizando o argumento pch para definir seu formato. Por fim, com a função text, adicionaremos textos ao gráfico, nas coordenadas escolhidas, por meio do argumento labels. Todas as funções aqui utilizadas pertencem ao pacote básico graphics. Para a função de verossimilhança, temos:\n\ncurve(expr = vero, \n      main = \"Gráfico da função de verossimilhança\", \n      xlab = \"Theta\", ylab = \"Verossimilhança\",\n      col = \"#32A0FF\",\n      ylim = c(0, 0.12))\nabline(v = 0.75, h = 0.10546875, lty = 2)\npoints(x = 0.75, y = 0.10546875, col = \"red\", pch = 20)\ntext(x = 0.86, y = 0.11, labels = c(\"(0.75, 0.10546875)\"))\n\n\n\n\n\n\n\n\nObserve que, de fato, a função de verossimilhança em questão possui um ponto de máximo absoluto em \\(\\theta = 0,75\\). Já quanto à função escore,\n\ncurve(expr = escore,\n      main = \"Gráfico da função escore\",\n      xlab = \"Theta\", ylab = \"Escore\",\n      col = \"#32A0FF\")\nabline(v = c(0, 0.75), h = 0, lty = c(1, 1, 2))\npoints(x = 0.75, y = 0, col = \"red\", pch = 20)\ntext(x = 0.805, y = 15, labels = c(\"(0.75, 0)\"))\n\n\n\n\n\n\n\n\nNote que, como esperado, o gráfico da função escore intercepta o eixo \\(x\\) apenas no ponto \\(\\theta = 0,75\\), o qual, como visto acima, é um ponto de máximo da função de verossimilhança. Dessa forma, podemos observar que os cálculos feitos ao longo do post estão de acordo com as representações visuais das funções envolvidas.\n\n\nConclusão\nAo longo desse post, discutimos sobre os conceitos de verossimilhança e função de verossimilhança, demonstrando algumas de suas utilizações dentro da Estatística. Algo que aqui não foi discutido, muito por conta da falta de conhecimento do autor sobre o assunto até o presente momento, é o papel da função de verossimilhança dentro da inferência Bayesiana. Mas isso é papo para outra hora. De qualquer forma, esperamos que as definições e os exemplos aqui apresentados tenham ficado claros, de forma que o leitor tenha conseguido adquirir algum conhecimento sobre essa temática. Para comentários, sugestões e colaborações científicas, entre em contato conosco através do email observatorioobstetricobr@gmail.com.\n\n\nReferências\nhttps://ruccs.rutgers.edu/images/personal-charles-r-gallistel/publications/2015-APS-Bayes-for-Beginners-1-Probability-and-Likelihood---Association-for-Psychological-Science.pdf\nhttps://fsbmat-ufv.github.io/blog_posts/14-08-2019/post1\nhttps://en.wikipedia.org/wiki/Likelihood_function\nhttps://online.stat.psu.edu/stat415/lesson/1/1.2\nMOOD, Alexander McFarlane. Introduction to the theory of statistics. 3. ed. EUA: McGraw-Hill Education, 1974. 480 p.\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{sant'ana herzog2022,\n  author = {Sant’Ana Herzog, Rafael},\n  title = {Função de Verossimilhança},\n  date = {2022-03-15},\n  url = {https://rafaherzog.github.io/rafasantanah/posts/2022-03-15-funcao-de-verossimilhanca/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSant’Ana Herzog, Rafael. 2022. “Função de Verossimilhança.”\nMarch 15, 2022. https://rafaherzog.github.io/rafasantanah/posts/2022-03-15-funcao-de-verossimilhanca/."
  },
  {
    "objectID": "posts/24-08-2023-o-valor-p/index.html",
    "href": "posts/24-08-2023-o-valor-p/index.html",
    "title": "O valor-p",
    "section": "",
    "text": "Introdução\nTestes de hipóteses estão, sem dúvida alguma, entre os métodos estatísticos mais proeminentes dentro da pesquisa científica. A possibilidade de se testar uma suposição a respeito do comportamento de uma ou mais variáveis aleatórias a partir de uma amostra de observações dessas variáveis abriu portas para a utilização da Estatística nos mais diversos campos da ciência, desde as áreas sociais, comportamentais e médicas, à área física, química e muitas outras. Ao longo deste post, entretanto, não trataremos especificamente de testes de hipóteses, mas sim de um elemento intrínseco a essas ferramentas: o valor-p. O valor-p (também chamado de p-valor ou valor de p) é uma probabilidade relacionada à amostra observada que fornece uma dimensão da “compatibilidade” dessa amostra com a hipótese nula que está sendo testada, nos dando, assim, uma base para a tomada de decisões. Ao longo desta publicação, trataremos da origem dessa ferramenta, de sua definição formal, dos aparentes benefícios de sua utilização - e dos malefícios de sua má-utilização - e discutiremos maneiras de se contornar suas limitações, que estão sendo cada vez mais evidenciadas dentro do meio científico. É importante dizer que este texto irá assumir que o leitor já possui conhecimentos prévios dos conceitos básicos relacionados à teoria dos testes de hipóteses. Assim, caso esse não seja o seu caso, ou caso deseje relembrar alguns desses conceitos, sugerimos a leitura do post a respeito de testes de hipóteses produzido pelo Observatório Obstétrico Brasileiro, disponível neste link.\n\n\nA origem do valor-p\nA origem do valor-p pode ser atribuída a Ronald Fisher, o famoso estatístico inglês considerado um dos pais da Estatística moderna. Em seu livro “Statistical Methods for Research Workers”, publicado em 1925, Fisher apresenta uma série de exemplos de experimentos em que calcula valores-p para o que chamava de testes de significância, um método de testagem de hipóteses desenvolvido por este autor que precede a teoria de testes de hipóteses desenvolvida por Jerzy Neyman e Egon Pearson, publicada em 1928 e difundida na grande maioria dos livros didáticos atuais. A ideia era utilizar a probabilidade fornecida pelo valor-p para se decidir sobre a significância dos resultados obtidos, ou seja, decidir se os dados poderiam ser utilizados, ou não, para se rejeitar a hipótese nula proposta, seja ela referente à igualdade entre os efeitos de tratamentos, à adequação de um conjunto de dados a uma certa distribuição de probabilidades ou à independência entre duas variáveis categóricas.\nA ideia por trás do valor-p é, de certa forma, simples (mas frequentemente mal entendida): ele representa a probabilidade de observarmos valores da estatística de teste tão extremos quanto o observado, sob a suposição de a hipótese nula (\\(H_0\\)) ser verdadeira. Dessa forma, quanto menor o valor-p, mais incompatíveis são os dados observados com a hipótese nula formulada; assim, quando o valor-p é suficientemente pequeno, encaramos essa incompatibilidade como sendo uma evidência de que a hipótese nula não é aceitável e, portanto, deve ser rejeitada. É comum lermos e dizermos que um valor-p é significativo quando ele é baixo o suficiente para se rejeitar uma hipótese nula. O sentido original da palavra “significativo”, entretanto, foi se modificando com o passar do tempo, como explica David Salsburg em seu livro “Uma senhora toma chá”, publicado em 2002: quando a teoria por trás do valor-p estava sendo desenvolvida, ser “significativo” representava apenas que o cálculo significou ou mostrou alguma coisa. O sentido atual da palavra, por outro lado, remete a algo ser muito importante, e é nesse sentido que muito se fala, erroneamente, que um certo resultado foi significativo. Como veremos posteriormente, a utilização dos termos “significativo” e “não significativo” está sendo, nos anos recentes, um dos vários pontos de debate dentro da comunidade estatística, com muitos defendendo abandonar a utilização dessas expressões, por conta da conotação potencialmente enganosa e perigosa que elas podem carregar.\nAté este ponto, nos limitamos a dizer que decidimos rejeitar uma hipótese nula caso o valor-p obtido seja suficientemente pequeno. Ser “suficientemente pequeno” é, entretanto, outra definição problemática dentro da Estatística. É discutível se Fisher definiu que valores-p menores que 0,05 são “significativos” e devem levar à rejeição da hipótese nula, mas é fato que esse número passou a ser visto como um grande critério universal para a tomada de decisões estatísticas. Experimentos que falham em atingir esse valor são frequentemente descartados, enquanto aqueles que o atingem são tratados como os que realmente importam. A atribuição da definição desse critério padrão do valor-p a Fisher se deve, possivelmente, a uma afirmação feita por este autor em seu artigo “The Statistical Method in Psychical Research”, publicado em 1929, no qual Fisher diz que “é prática comum julgar um resultado significativo se ele é de tal magnitude que possa ser reproduzido por acaso não mais frequentemente que uma vez em 20 tentativas” e que “esse é um nível arbitrário, mas conveniente, de significância para o investigador prático”. Nesse mesmo artigo, Fisher diz, ainda, que “o teste de significância só informa o que ignorar, a saber, todos os experimentos nos quais resultados significativos não são obtidos” e que “o investigador deveria apenas afirmar que um fenômeno é experimentalmente demonstrável quando sabe como planejar um experimento de forma que raramente falhe em dar um resultado significativo”. Dessa forma, de acordo com o autor, “resultados significativos isolados, que ele não sabe como reproduzir, são deixados em suspenso para futura investigação”.\nComo podemos notar, Fisher considera que a significância de um resultado só está realmente demonstrada se a grande maioria de um conjunto de experimentos idênticos se mostra individualmente significante. Essa definição, entretanto, se perdeu ao longo do tempo, e definitivamente não é considerada na maior parte dos estudos e pesquisas científicas. Por outro lado, existe uma grande problemática na fala de que “o teste de significância só informa o que ignorar, a saber, todos os experimentos nos quais resultados significativos não são obtidos”, uma vez que falhar em obter a significância não é prova de que o efeito que se deseja mostrar não existe. Mas, ponhamos em pausa essa discussão para o presente momento - voltaremos a ela numa seção específica - e partamos para a apresentação de como incorporamos o valor-p à teoria de testes de hipóteses que estamos acostumados a conhecer.\n\n\nA utilização do valor-p em testes de hipóteses\nQuando temos o primeiro contato com a teoria relacionada a testes de hipóteses, é comum que, para decidirmos quanto à rejeição ou não de uma hipótese nula, sigamos o seguinte processo:\n\ndefinimos a hipótese nula a ser testada, \\(H_0\\), bem como a hipótese alternativa, \\(H_1\\);\ndefinimos a estatística que será utilizada para se testar a hipótese nula, cuja distribuição de probabilidades assumindo \\(H_0\\) como verdadeira deve ser conhecida;\nfixamos o nível de significância \\(\\alpha\\) do teste, ou seja, a probabilidade de rejeitarmos a hipótese nula quando ela é verdadeira;\nconstruímos, com base no nível de significância adotado e assumindo \\(H_0\\) como verdadeira, a região crítica, que representa o conjunto de valores da estatística de teste que levariam à rejeição da hipótese nula caso fossem observados;\ncalculamos, utilizando as observações obtidas em uma amostra, o valor da estatística de teste;\ncaso o valor observado da estatística de teste não pertença à região crítica, não rejeitamos \\(H_0\\); caso contrário, a hipótese nula é rejeitada.\n\nComo podemos perceber, o procedimento padrão de um teste de hipóteses parte da fixação de um nível de significância \\(\\alpha\\), sendo a construção da região crítica dependente desse valor. Assim, caso fosse de nosso interesse verificar se a hipótese nula seria rejeitada sob um valor diferente de \\(\\alpha\\), teríamos de reconstruir a região de rejeição. Fazendo uso do valor-p para a tomada de decisões, por outro lado, a fixação do nível de significância de antemão e a construção da região crítica já não são mais necessárias: depois de calculado o valor da estatística de teste, podemos encontrar a probabilidade de observarmos valores dessa estatística tão extremos quanto o observado sob a suposição de \\(H_0\\) ser verdadeira - ou seja, o valor-p-, rejeitando a hipótese nula caso essa probabilidade seja menor que um certo valor de \\(\\alpha\\). É importante notar que, por mais que a fixação do nível de significância de antemão não seja estritamente necessária para a realização de um teste de hipóteses que utiliza o valor-p, é recomendado que o valor de \\(\\alpha\\) seja, sim, definido de forma prévia à realização do teste, para que o pesquisador não seja induzido a mudar seu critério de rejeição de acordo com os resultados obtidos.\nPara fixarmos os conceitos aqui discutidos, vejamos um exemplo simples. Suponha que, após 100 lançamentos de uma moeda, 22 caras tenham sido observadas, e que seja de nosso interesse verificar se essa moeda é honesta utilizando um nível de significância de 1%. Reescrevendo esse problema de maneira formal, seja \\(X\\) a variável aleatória que conta o número de caras que ocorreram ao longo dos 100 lançamentos da moeda. Sabemos, então, que \\(X\\) segue distribuição binomial com parâmetros \\(n = 100\\) e \\(p\\), sendo \\(p\\) - a probabilidade de se observar uma cara - um valor desconhecido. Como é de nosso desejo verificar se a moeda é honesta, e como sabemos que uma moeda honesta tem probabilidade 0,5 de resultar em cara ou coroa, queremos testar as seguintes hipóteses:\n\\[H_0: p = 0,5 \\quad \\text{vs} \\quad H_1: p &lt; 0,5.\\]\nNote que, como observamos poucas caras na amostra, podemos suspeitar de que a probabilidade de se obter uma cara por meio dessa moeda seja menor que a probabilidade de 0,5 que esse evento teria caso a moeda fosse honesta e, por isso, definimos a hipótese alternativa de forma a obtermos um teste unilateral à esquerda. Dessa forma, caso rejeitássemos a hipótese nula, poderíamos dizer que teríamos evidências para acreditar que a probabilidade de se obter uma cara a partir da moeda em questão é menor que 0,5. Para a resolução deste problema, precisamos, primeiramente, definir a estatística de teste a ser utilizada. Como estamos tratando de uma proporção, utilizar o estimador da proporção amostral, dado por\n\\[\n\\hat{P} = \\frac{X}{n},\n\\]\nparece ser uma boa pedida, em especial porque conhecemos sua distribuição aproximada sob \\(H_0\\). Calculando, assim, o valor dessa estatística, sabendo que \\(n = 100\\) e que o número observado de caras foi de \\(x = 37\\), obtemos\n\\[\\hat{p} = 0,37.\\]\nDeterminado o valor observado da estatística de teste, precisamos, então, calcular o valor-p. Como o tamanho da amostra é suficientemente grande, sabemos, como consequência do Teorema do Limite Central e de forma aproximada, que\n\\[\n\\hat{P} \\sim N\\left(p; \\frac{p(1 - p)}{n}\\right).\n\\]\nSupondo \\(H_0\\) verdadeira, ou seja, que \\(p = 0,5\\), a distribuição de \\(\\hat{P}\\) fica completamente especificada, a saber,\n\\[\n\\hat{P} \\sim N\\left(0,5; 0,0025\\right).\n\\]\nDessa forma, como encontramos \\(\\hat{p} = 0,37\\), podemos calcular, padronizando a variável aleatória \\(\\hat{P}\\), a probabilidade de observarmos valores tão extremos quanto esse supondo que a moeda seja honesta, ou seja,\n\\[\n\\begin{align}\nP(\\hat{P} \\leq 0,37 | p = 0,5) & = P\\left(\\frac{\\hat{P} - 0,5}{\\sqrt{0,0025}} \\leq \\frac{0,37 - 0,5}{\\sqrt{0,0025}} \\Bigg{|} p = 0,5 \\right) \\\\\n                     & = P\\left(Z \\leq \\frac{-0,13}{0,05} \\Bigg{|} p = 0,5 \\right) \\\\\n                     & = P(Z \\leq -2,6 | p = 0,5) \\\\\n                     & = 0,005.\n\\end{align}\n\\] Como o valor-p obtido foi de 0,005, podemos dizer que, se a moeda fosse realmente honesta, a probabilidade de encontrarmos uma amostra de 100 lançamentos com 37% ou menos ocorrências de caras seria de 0,5%. Isso sugere que, ou estamos diante de uma amostra muito rara, que deve ocorrer 1 vez a cada 200 realizações do experimento, ou que a hipótese nula formulada não é aceitável. Como adotamos um nível de significância de 1%, e como o valor-p obtido foi menor que esse nível de significância, somos levados a decidir pela rejeição da hipótese nula. Dessa forma, temos evidências para acreditar que a moeda em questão não é honesta (ela, de fato, não era; o valor 37 foi gerado aleatoriamente a partir da distribuição binomial com \\(p = 0,3\\), utilizando a semente 42 e a função rbinom(), disponível no pacote stats{}, do R).\nPara finalizar esta seção, uma observação importante a se fazer é a respeito do cálculo do valor-p para testes de hipóteses bilaterais, ou seja, aqueles cuja hipótese alternativa é do tipo \\(\\theta \\neq \\theta_0\\), sendo \\(\\theta\\) um parâmetro qualquer. Nesses casos, um possível procedimento é tomar o valor-p bilateral como sendo o dobro do valor-p unilateral. Quando a distribuição da estatística de teste sob \\(H_0\\) é simétrica, como no caso da distribuição normal e da distribuição t de Student, esse procedimento é razoável. Em outras situações, pode ser preferível relatar o valor do valor-p unilateral e a direção segundo a qual a observação se afasta de \\(H_0\\). Para mais detalhes, sugerimos a leitura da seção referente ao valor-p dentro do livro “Estatística básica”, escrito por Bussab e Morettin e publicado em 2010.\n\n\nConsiderações a respeito da utilização do valor-p\nEm sua superfície, a utilização do valor-p fornece uma maneira padronizada e objetiva de se quantificar a evidência estatística, aparentando impedir que julgamentos subjetivos e outros tipos de vieses influenciem as conclusões do pesquisador (apesar de a própria escolha de um certo nível de significância, por exemplo, ser uma decisão subjetiva). Além disso, a praticidade oferecida pelo valor-p pesa positivamente para a continuidade de sua utilização, uma vez que, por se tratar aparentemente de um único número que resume toda uma análise, ele é de fácil comunicação e reporte. Por essas e outras razões, a utilização do valor-p se espalhou fortemente dentro do meio científico, tornando cada vez maior a quantidade de publicações que fazem mal uso dessa medida. Um dos erros mais comumente cometidos é referente à interpretação do significado do valor-p: diferente do que pesquisadores desavisados possam acreditar, o valor-p não fornece a probabilidade de a hipótese nula ser falsa - até porque, dentro da Inferência Clássica, calcular a probabilidade de um parâmetro estar dentro de um intervalo de valores não faz sentido, uma vez que, aqui, parâmetros são tratados como constantes.\nAlém disso, como nota o próprio Fisher ao comentar sobre o cálculos de valores-p para testes qui-quadrado em seu livro “Statistical Methods for Research Workers”, essa medida não representa o grau da associação entre duas variáveis ou o grau da diferença entre dois efeitos; ela fornece apenas uma ideia de se essas associações ou diferenças são ou não de uma magnitude que pode ser atribuída ao acaso. Dessa forma, valores-p pequenos não definem, necessariamente, que um resultado encontrado tem grande relevância prática. Tomando como exemplo um estudo que busque encontrar diferenças entre tratamentos, por mais que o valor-p possa revelar a existência de evidências para se acreditar que dois efeitos são estatisticamente diferentes, essa diferença pode ser tão pequena a ponto de, na prática, ela não ser realmente significativa. Qualquer efeito, não importa quão mínimo, pode produzir um valor-p pequeno se o tamanho da amostra e a precisão das medições forem grandes o suficiente, e efeitos grandes podem produzir valores-p pouco impressionantes caso a amostra seja pequena e as medições forem imprecisas. Dessa forma, a utilização única e exclusivamente do valor-p para se decidir sobre a “significância” de um experimento vem sendo cada vez mais criticada, em especial porque, como já citado ao longo do texto, o termo “significante” deixou de denotar que um resultado merece uma investigação mais aprofundada e passou a ser sinônimo de importância científica.\nA discussão a respeito da má utilização do valor-p chegou, possivelmente, ao seu ápice em 2016, quando a American Statistical Association (ASA) publicou o artigo “The ASA Statement on p-Values: Context, Process, and Purpose”, que contém uma discussão a respeito do verdadeiro significado do valor-p e dos princípios relacionados a sua utilização. Desde então, a comunidade estatística passou a enfatizar cada vez mais a importância de se complementar - ou mesmo de se substituir - a utilização do valor-p, sendo muito recomendada a utilização de métodos que focalizem o processo de estimação, tais como intervalos de confiança e de predição, razões de verossimilhança e métodos Bayesianos, como intervalos de credibilidade e fatores de Bayes. Mesmo que todas essas práticas sejam dependentes de outras suposições, elas podem fornecer mais diretamente a dimensão do tamanho de um efeito - e de sua incerteza associada - ao invés de focar apenas na questão de uma hipótese ser “correta” ou não.\nEm suma, é no mínimo discutível que um único número substitua todo o processo científico que está por trás de uma análise estatística, e a insistência nesse erro é um grande empecilho para uma boa prática científica, para a qual boas práticas estatísticas são um componente essencial. Dessa forma, ao invés de se confiar unicamente no valor-p - que tem, é claro, a sua utilidade - para se decidir sobre a relevância de um estudo, um bom pesquisador deve focar em entender os princípios de um bom planejamento experimental, do valor que análises descritivas têm dentro de um estudo estatístico, da importância de se entender o fenômeno em estudo e da necessidade de se interpretar os resultados de uma análise dentro do contexto correto, de forma a se alcançar o verdadeiro entendimento a respeito de todas as medidas encontradas ao longo de todo o processo realizado.\n\n\nReferências\nFisher, Ronald A. “Statistical methods for research workers”. Breakthroughs in statistics: Methodology and distribution. New York, NY: Springer New York, 1970. 66-70.\nFisher, Ronald A. “The Statistical Method in Psychical Research”. Proceedings of the Society for Psychical Research, 38, 189-192.\nNeyman, J. e Pearson, E. S. (1928). “On the use and interpretation of certain test criteria for purposes of statistical inference”. Biometrika, 20A, 175–240.\nMorettin, P. A. e Bussab, W. de O. Estatística Básica. 10. ed. São Paulo: Saraiva, 2010.\nSalsburg, David S. Uma senhora toma chá… como a estatística revolucionou a ciência no século XX. Rio de Janeiro: Zahar, 2009.\nWasserstein, Ronald L., and Nicole A. Lazar. “The ASA statement on p-values: context, process, and purpose.” The American Statistician, 70.2 (2016): 129-133.\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{sant'ana herzog2023,\n  author = {Sant’Ana Herzog, Rafael},\n  title = {O Valor-*p*},\n  date = {2023-08-24},\n  url = {https://rafaherzog.github.io/rafasantanah//posts/25-01-2022-rmm/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSant’Ana Herzog, Rafael. 2023. “O Valor-*p*.” August 24,\n2023. https://rafaherzog.github.io/rafasantanah//posts/25-01-2022-rmm/."
  },
  {
    "objectID": "posts/25-01-2022-rmm/index.html",
    "href": "posts/25-01-2022-rmm/index.html",
    "title": "Entendendo, calculando e visualizando a razão de mortalidade materna do Brasil com o R",
    "section": "",
    "text": "Introdução\nNeste post, iremos discutir sobre o conceito de razão de mortalidade materna, utilizando o R para a realização de seu cálculo e para a visualização de sua evolução ao longo do tempo no Brasil. Em primeiro lugar, chama-se de razão de mortalidade materna (RMM) o indicador utilizado para conhecer o nível de morte materna, calculado pela razão entre o número de óbitos maternos e o número de nascidos vivos durante um período de tempo e em um determinado espaço geográfico, multiplicada por 100.000 (padrão internacional). Além disso, é considerado como “óbito materno” a “morte de uma mulher durante a gestação, parto ou dentro de um período de 42 dias após o término da gestação (o puerpério) (…), devida a qualquer causa relacionada com ou agravada pela gravidez ou por medidas em relação a ela, porém não devida às causas acidentais ou incidentais” (OMS, 1997). Matematicamente, temos que a razão de mortalidade materna é dada por:\n\\[\\frac{\\text{Nº de óbitos maternos em um dado período de tempo e espaço geográfico}}{\\text{Nº de nascidos vivos sob essas mesmas condições}} \\times 100.000\\]\nO resultado do cálculo acima deve ser interpretado como sendo o número de óbitos maternos em um dado período de tempo e em um certo espaço geográfico a cada 100.000 nascidos vivos nessas mesmas condições. Apesar de, em teoria, ser mais correta a utilização do número total de gestações para o denominador dessa razão, a impossibilidade de obtenção desse dado faz com que utilizemos em seu lugar o número total de nascidos vivos, tornando o resultado encontrado para a taxa uma aproximação do seu verdadeiro valor. É também importante destacar que, diferentemente do Ministério da Saúde, que realiza correções no cálculo da razão de mortalidade materna para levar em consideração a subenumeração de mortes maternas e de nascidos vivos, trabalharemos nesse post com a razão de mortalidade materna em seu estado natural, sem maiores modificações.\nCom esses conceitos iniciais em mente, passemos, agora, para o cálculo dessa razão no R, bem como para diferentes formas de como podemos visualizar sua evolução no país ao longo dos anos.\n\n\nCarregando os pacotes necessários e importando a base de dados\nPara a realização do cálculo da razão de mortalidade materna no R, precisamos, primeiramente, carregar o pacote tidyverse, formado por uma coleção de outros pacotes que nos serão úteis, em especial o dplyr, o readr e o ggplot2. Carregaremos, também, os pacotes sf e DT, que serão utilizados, respectivamente, para a leitura de um arquivo do tipo shapefile e para a construção de uma tabela, ambos no último exemplo do texto.\n\n# Carregando os pacotes necessários\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(DT)\n\nPara importar a base de dados com a qual trabalharemos ao longo do post, será utilizada a função read_delim, do pacote readr, que receberá dois argumentos: o primeiro, file, recebe o nome do arquivo .csv que contém nossa base de dados; já o segundo argumento, delim, recebe o delimitador utilizado no arquivo para separar suas colunas, que nesse caso é o ponto e vírgula. A base de dados será salva no objeto nomeado por dados_maternos. Caso queira, você também pode realizar o download do arquivo através deste link.\n\ndados_maternos &lt;- read_delim(file = \"dados_maternos.csv\", delim = \";\")\n\nA tibble dados_maternos é formada por seis colunas, que contêm, respectivamente, o código do estado, sua sigla, a região à qual o estado pertence, o ano ao qual a linha se refere (indo de 2005 a 2019, uma vez que os dados de 2020 e 2021 ainda são preliminares), o número de nascidos vivos no estado no dado ano e o número de óbitos maternos no mesmo. As linhas da tibble estão ordenadas de acordo com o código das unidades federativas. Os dados referentes aos nascidos vivos são provenientes do SINASC (Sistema de Informações sobre Nascidos Vivos) e foram obtidos através do Painel de Monitoramento de Nascidos Vivos do Ministério da Saúde, enquanto as informações sobre os óbitos maternos são provenientes do SIM (Sistema de Informações sobre Mortalidade) e foram obtidas por meio do Painel de Monitoramento da Mortalidade Materna do mesmo órgão. Observe abaixo as dez primeiras linhas da tibble.\n\ndados_maternos\n\n# A tibble: 405 × 6\n   codigo_uf uf    regiao   ano nascimentos obitos\n       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1        11 RO    Norte   2005       28081     15\n 2        11 RO    Norte   2006       24925     15\n 3        11 RO    Norte   2007       22996     10\n 4        11 RO    Norte   2008       26791     12\n 5        11 RO    Norte   2009       26083     17\n 6        11 RO    Norte   2010       25835     16\n 7        11 RO    Norte   2011       27658     15\n 8        11 RO    Norte   2012       26513     17\n 9        11 RO    Norte   2013       27097     18\n10        11 RO    Norte   2014       27560     22\n# ℹ 395 more rows\n\n\n\n\nCalculando a razão de mortalidade materna e a adicionando à base de dados\nCaso nosso interesse seja apenas adicionar uma coluna contendo a razão de mortalidade materna para cada estado e ano à nossa base de dados, precisamos apenas executar um comando. Utilizando a função mutate, do pacote dplyr, adicionaremos uma nova coluna à tibble dados_maternos, a qual nomearemos por “mortalidade”. Essa nova coluna será formada por um vetor de números no qual cada posição representa o valor do cálculo discutido na seção anterior para cada uma das linhas de dados_maternos, arredondados para duas casas decimais por meio da função round, do pacote básico base. Por padrão, a função mutate irá manter todas as colunas originais da tibble utlizada, adicionando a nova coluna em seu lado direito.\n\n# Calculando a razão de mortalidade materna e a adicionando à base de dados\ndados_maternos &lt;- dados_maternos |&gt; \n  mutate(\"mortalidade\" = round((dados_maternos$obitos/dados_maternos$nascimentos) * 100000, digits = 2))\n\nAs primeiras linhas do objeto resultante desse processo podem ser vistas a seguir:\n\ndados_maternos\n\n# A tibble: 405 × 7\n   codigo_uf uf    regiao   ano nascimentos obitos mortalidade\n       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n 1        11 RO    Norte   2005       28081     15        53.4\n 2        11 RO    Norte   2006       24925     15        60.2\n 3        11 RO    Norte   2007       22996     10        43.5\n 4        11 RO    Norte   2008       26791     12        44.8\n 5        11 RO    Norte   2009       26083     17        65.2\n 6        11 RO    Norte   2010       25835     16        61.9\n 7        11 RO    Norte   2011       27658     15        54.2\n 8        11 RO    Norte   2012       26513     17        64.1\n 9        11 RO    Norte   2013       27097     18        66.4\n10        11 RO    Norte   2014       27560     22        79.8\n# ℹ 395 more rows\n\n\n\n\nAnalisando a evolução da razão de mortalidade materna nacional através de um gráfico de linhas\nAgora que já sabemos como calcular a razão de mortalidade materna e como adicioná-la à nossa base de dados, podemos passar para algumas análises e manipulações mais interessantes. O primeiro exemplo tratará de uma visão nacional do indicador. Para realizar uma análise de sua evolução ao longo dos anos, utilizaremos um gráfico de linhas, tipo de gráfico muito útil quando o objetivo é observar o comportamento de séries temporais. Primeiramente, precisamos agrupar nossa base de dados por ano, uma vez que queremos analisar todos os estados de forma conjunta. Para isso, será utilizada a função group_by, do pacote dplyr, na qual o primeiro argumento é o data.frame, ou tibble, no(a) qual a operação será realizada, e o segundo argumento é o nome da variável pela qual queremos que os dados sejam agrupados. Caso tivéssemos interesse em agrupá-los por mais de uma variável, bastaria adicionar os nomes das outras variáveis após “ano”, separados por vírgula.\n\n# Agrupando os dados por ano e os resumindo\ndados_maternos_nacional &lt;- group_by(dados_maternos, ano) |&gt;\n  summarise(nascimentos_br = sum(nascimentos),\n            obitos_br = sum(obitos),\n            mortalidade_br = round((obitos_br/nascimentos_br) * 100000, 2)) \n\nFeito o agrupamento, utilizamos a função summarise, também do pacote dplyr, para realizar a criação de uma nova tibble, que será como um resumo de dados_maternos. O número de linhas dessa tibble será igual ao número de combinações possíveis das variáveis agrupadas no passo anterior; como agrupamos apenas por ano, a nova tibble terá 15 linhas, uma para cada ano diferente presente na base de dados utilizada. Os argumentos de summarise serão pares de nomes e funções, nos quais os primeiros serão os nomes das colunas que irão compor a nova tibble, e as segundas são as funções que serão aplicadas na variável indicada da tibble original. Assim, criaremos três novas colunas: nascimentos_br, que receberá a soma de todos os nascidos vivos no dado ano; obitos_br, que receberá a soma de todos os óbitos maternos no dado ano; e mortalidade_br, que receberá a razão de mortalidade materna do período.\nNote que não tivemos que dizer qual o data.frame (ou tibble) que seria utilizado nessa função, visto que estamos utilizando o operador pipe (|&gt;) para encadear as funções group_by e summarise. O resultado desse código, guardado no objeto dados_maternos_nacional, é uma tibble cujas primeiras linhas podem ser vistas a seguir:\n\ndados_maternos_nacional\n\n# A tibble: 15 × 4\n     ano nascimentos_br obitos_br mortalidade_br\n   &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1  2005        3035096      1620           53.4\n 2  2006        2944928      1623           55.1\n 3  2007        2891328      1590           55.0\n 4  2008        2934828      1681           57.3\n 5  2009        2881581      1872           65.0\n 6  2010        2861868      1719           60.1\n 7  2011        2913160      1610           55.3\n 8  2012        2905789      1583           54.5\n 9  2013        2904027      1686           58.1\n10  2014        2979259      1739           58.4\n11  2015        3017668      1738           57.6\n12  2016        2857800      1670           58.4\n13  2017        2923535      1718           58.8\n14  2018        2944932      1658           56.3\n15  2019        2849146      1576           55.3\n\n\nCom os dados preparados, podemos partir para a construção do gráfico de linhas, na qual utilizaremos a função ggplot, do pacote ggplot2. Atribuiremos ao argumento data a tibble dados_maternos_nacional, e ao argumento mapping os aesthetics do nosso gráfico, que nesse caso serão apenas o eixo x, que receberá a variável ano, e o eixo y, que receberá a variável mortalidade_br. O primeiro complemento do gráfico será a função geom_line, que criará a(s) linha(s) do gráfico de linhas. Os únicos argumentos que utilizaremos nessa função serão o color, para alterar a cor da linha a ser gerada, e o size, para alterar sua expessura. O próximo complemento, geom_point, é utilizado para fazer gráficos de dispersão, e criará pontos em nosso gráfico para facilitar sua visualização. Em scale_x_continuous, definimos que as marcações do eixo x devem ocorrer de ano em ano. O título do gráfico, sua legenda e os nomes dos eixos são atribuídos pelo complemento labs, e utilizaremos theme apenas para alterar o tamanho dos textos do gráfico.\n\n# Construindo o gráfico de linhas\nggplot(data = dados_maternos_nacional, mapping = aes(x = ano, y = mortalidade_br)) +\n  geom_line(color = \"#32A0FF\", size = 1) +\n  geom_point() +\n  scale_x_continuous(breaks = unique(dados_maternos_nacional$ano)) +\n  labs(title = \"Evolução da razão de mortalidade materna brasileira ao longo do tempo\",\n       caption = \"Fontes: SIM e SINASC\",\n       x = \"Ano\",\n       y = \"Razão de mortalidade materna\") +\n  theme(text = element_text(size = 16)) \n\n\n\n\n\n\n\n\nUma análise rápida do gráfico nos permite fazer um questionamento óbvio: o que aconteceu de 2008 para 2009 para que a razão de mortalidade materna nacional tivesse um aumento tão significativo em seu valor? A resposta, um assunto que está atualmente em alta: uma pandemia, que nesse caso era do vírus da Influenza A (H1N1). Observe o próximo exemplo para ter uma melhor noção dos efeitos dessa pandemia na razão de mortalidade materna em uma escala estadual.\n\n\nUtilizando gráficos de barras para visualizar os efeitos da pandemia de H1N1 nos estados brasileiros\nIniciada no México em abril de 2009, a pandemia do vírus da Influnza A, assim declarada em junho do mesmo ano pela OMS (G1, 2009), espalhou-se rapidamente pelo mundo. No Brasil, até dezembro de 2010, foram confirmados 59.867 casos e 2.173 mortes (da população geral) pelo vírus, de acordo com dados do Ministério da Saúde. Gestantes, entretanto, estão mais suscetíveis às complicações e aos óbitos por doenças infecciosas, como é o caso da Influenza. Dessa forma, mesmo que esse grupo de mulheres tenha sido precocemente identificado como um grupo de risco durante a pandemia de H1N1, houve, no ano de 2009, um aumento no número de óbitos maternos (e consequentemente, na razão de mortalidade materna, como observado no gráfico anterior), visto que a campanha nacional de vacinação contra o vírus foi iniciada apenas em março de 2010.\nPara este exemplo, digamos que seja de nosso interesse realizar a seguinte análise: observar a evolução das razões de mortalidade materna por estado nos anos de 2008 (pré-pandemia), 2009 (durante a pandemia), 2010 (início da vacinação) e 2011 (pós-pandemia), utilizando para isso uma série de gráficos de barras. O primeiro passo, como no exemplo anterior, é a preparação dos dados. Para realizá-la, utilizaremos a função filter, do pacote dplyr que filtrará no banco de dados designado apenas as linhas em que todas as condições dadas forem satisfeitas. Dessa forma, os argumentos dessa função devem ser expressões que retornem um valor lógico. No nosso caso, queremos filtrar as linhas da tibble dados_maternos nas quais a variável ano está entre 2008 e 2011, considerando os extremos. A tibble resultante dessa operação será salva em dados_maternos_h1n1.\n\n# Filtrando apenas os dados de 2008 a 2010\ndados_maternos_h1n1 &lt;- dados_maternos |&gt;\n  filter(ano &gt;= 2008 & ano &lt;= 2011) \n\nA construção do gráfico, apesar de também ser feita através do pacote ggplot2, é significativamente diferente àquela feita no exemplo anterior. Dentro da função ggplot, o argumento data receberá a tibble criada acima, enquanto os aesthetics do gráfico serão três: no eixo x, queremos a variável uf; no eixo y, a variável mortalidade; por fim, o argumento fill determinará de que forma cada barra do gráfico será preenchida, e queremos que seja de acordo com a variável ano. Note que estamos transformando essa última variável numa variável categórica por meio da função as.factor.\nO primeiro complemento do gráfico é a função geom_col, a qual construirá nossos gráficos de barras e que possui dois argumentos: position = “dodge”, que faz com que as barras construídas para cada estado sejam postas lado a lado, facilitando sua visualização, e width = 0.5, que muda a expessura das barras. O próximo complemento, facet_wrap, será responsável pela mágica desse exemplo. Imagine como seria péssima a visualização de 27 gráficos de barras, com quatro barras cada, numa única linha. Lembre-se, entretanto, que a nossa tibble dados_maternos_h1n1 possui uma coluna que especifica a região de cada estado. Assim, utilizaremos a função facet_wrap para que nosso gráfico contenha cinco fileiras de gráficos de barras, separando os gráficos de cada estado de acordo com a região à qual pertencem. Os argumentos que iremos utilizar são três: facets = vars(regiao) indicará que o gráfico deve ser separado pela variável regiao; scales = “free_x” torna as escalas do eixo x livres, fazendo com que os únicos valores exibidos no eixo x de cada gráfico sejam os estados que pertencem à dada região; o último, ncol = 1, define que o gráfico deve ter apenas uma coluna. Com o complemento scale_fill_manual, definiremos cores específicas para cada um dos quatro anos presentes no gráfico. Por fim, mudamos a posição da legenda do gráfico para o topo, utilizando o argumento legend.position = “top”, do complemento theme(). O gráfico resultante de todo esse processo pode ser visto abaixo.\n\n# Construindo os gráficos de barras\nggplot(data = dados_maternos_h1n1, \n       mapping = aes(x = uf, y = mortalidade, fill = as.factor(ano))) +\n  geom_col(position = \"dodge\", width = 0.5) +\n  facet_wrap(facets = vars(regiao), scales = \"free_x\", ncol = 1) + \n  labs(title = \"Efeitos da pandemia de H1N1 na razão de mortalidade materna dos estados brasileiros\", \n       caption = \"Fontes: SIM e SINASC\",\n       x = NULL, \n       y = \"Razão de mortalidade materna\",\n       fill = \"Ano\") +\n  scale_fill_manual(values = c(\"#377EB8\", \"#B2182B\", \"#4DAF4A\", \"#FD8D3C\")) +\n  theme(legend.position = \"top\", text = element_text(size = 16)) \n\n\n\n\n\n\n\n\nAnalisando os gráficos acima, podemos realizar algumas observações. Em primeiro lugar, 15 dos 27 estados brasileiros apresentaram um aumento na razão de mortalidade materna em 2009, incluindo todos os quatro estados da região Sudeste. Já em 2010, ano em que foi iniciada a campanha de vacinação nacional, podemos observar um efeito contrário: 18 dos 27 estados tiveram o valor de sua razão diminuído, tendência que também pode ser observada em 2011, primeiro ano pós-pandemia, no qual 19 unidades federativas apresentaram uma redução em sua razão de mortalidade materna. Apesar de não podermos afirmar com plena certeza, sem análises mais profundas, que a pandemia da Influenza A, bem como seu declínio e o início da vacinação, são os responsáveis pelos efeitos citados observando apenas os gráficos feitos, podemos, no mínimo, considerá-los como fortes candidatos. Mas passemos agora para o próximo exemplo. Vamos falar sobre as diferenças regionais do Brasil no que tange à razão de mortalidade materna.\n\n\nObservando a heterogeneidade das regiões brasileiras por meio de um gráfico de linhas\nNão é segredo que o território brasileiro é marcado por intensas diferenças socioeconômicas nas mais diversas escalas geográficas. Quando pensamos nas regiões do país, sabemos que, historicamente, uma série de fatores como a concentração industrial, a ocupação tardia e aspectos naturias contribuíram para o desenvolvimento desigual das mesmas, o que pode ser constatado através da comparação entre vários tipos de indicadores das cinco regiões brasileiras. Tomando como exemplo o IDHM (Índice de Desenvolvimento Humano Municipal), temos, de acordo com dados do PNUD (Programa das Nações Unidas para o Desenvolvimento), que em 2016 as regiões Norte e Nordeste possuíam os piores índices do país, atingindo, respectivamente, 0,667 e 0,663, números consideravelmente mais baixos do que os das regiões Sudeste (0,766), Centro-Oeste (0,757) e Sul (0,754). No que diz respeito à razão de mortalidade materna, veremos neste exemplo que essa tendência se repete, utilizando, para isso, um gráfico de (várias) linhas que nos permitirá analisar como se dá a desigualdade desse indicador entre as cinco regiões do Brasil.\nO primeiro passo, como sempre, é preparar os dados. De forma semelhante ao exemplo anterior, vamos agrupar os dados utilizando a função group_by e os resumir com summarise. A diferença é que aqui estaremos agrupando a tibble dados_maternos por região e ano, ao invés de agrupá-los somente por ano.\n\n# Agrupando os dados por região e ano e os resumindo\ndados_maternos_regional &lt;- group_by(dados_maternos, regiao, ano) |&gt;\n  summarise(nascimentos_reg = sum(nascimentos),\n            obitos_reg = sum(obitos),\n            mortalidade_reg = round((obitos_reg/nascimentos_reg) * 100000, 2))\n\nQuanto ao gráfico, utilizaremos para o argumento data, da função ggplot, a tibble criada no passo anterior, enquanto os aesthetics de mapping serão três: no eixo x, deve ser representada a variável ano; no eixo y, a variável mortalidade_reg; quanto ao argumento color, queremos que as linhas sejam coloridas de acordo com a variável regiao. O complemento geom_line criará as linhas do gráfico, cuja expessura é alterada utilizando o argumento color. Em scale_x_continuous, definiremos com o argumento breaks que as marcações do eixo x devem ocorrer em intervalos de um ano. Com o complemento geom_text, escreveremos ao final das linhas a respectiva região que cada linha representa. Para isso, no argumento data, filtraremos nossa tibble para manter apenas as linhas em que o ano é o último, ou seja, 2019. Os aesthetics de mapping serão os seguintes: em label, onde devemos dizer o que será escrito no gráfico, escolhemos a variável regiao, o que fará com que sejam escritos os nomes das cinco regiões; em x, definimos as coordenadas do eixo x em que serão escritos os textos determinados em label, os quais deverão estar levemente à direita do último ano de registro; em y, designamos as coordenadas do eixo y nas quais os textos serão escritos, e queremos que eles estejam na altura da razão de mortalidade materna do último ano; por fim, definimos em color que os textos devem ser coloridos de acordo com a variável regiao. O argumento clip = “off”, do complemento coord_cartesian, garante que os nomes das regiões não sejam cortados caso ultrapassem os limites do gráfico. Veja a seguir o gráfico resultante desse código.\n\n# Construindo o gráfico de linhas\nggplot(data = dados_maternos_regional, \n       mapping = aes(x = ano, \n                     y = mortalidade_reg,\n                     color = regiao)) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = unique(dados_maternos_regional$ano)) +\n  labs(title = \"Evolução da razão de mortalidade materna das cinco regiões brasileiras ao longo do tempo\",\n       caption = \"Fontes: SIM e SINASC\",\n       x = \"Ano\",\n       y = \"Razão de mortalidade materna\") +\n  geom_text(data = dados_maternos_regional |&gt; filter(ano == last(ano)), \n            mapping = aes(label = regiao,\n                          x = ano + 0.75,\n                          y = mortalidade_reg, \n                          color = regiao),\n            size = 4.5) +\n  coord_cartesian(clip = \"off\") +\n  theme(legend.position = \"none\", text = element_text(size = 16))\n\n\n\n\n\n\n\n\nObservando o gráfico acima, podemos destacar alguns pontos de interesse. Em primeiro lugar, é possível observar que em todos os anos analisados as regiões Norte ou Nordeste apresentaram as maiores razões de mortalidade materna do país, enquanto as regiões Sul ou Sudeste só não estiveram na outra ponta desse espectro no ano de 2007, quando a região Centro-Oeste apresentou o menor índice do Brasil. Além disso, outro ponto notório é o aumento expressivo nos valores desse indicador de 2008 para 2009 em quatro das cinco regiões brasileiras, assunto que já exploramos anteriormente. Por fim, podemos salientar a diminuição contínua na razão de mortalidade materna nordestina a partir de 2013, atingindo em 2019 níveis comparáveis aos da região Centro-Oeste. Vamos agora para o último exemplo deste post, no qual faremos uma breve análise do cenário atual da razão de mortalidade materna no país.\n\n\nAnalisando o cenário atual da razão de mortalidade materna brasileira por meio de um mapa coroplético\nNo ano de 2000, foram estabelecidos através da Declaração do Milênio da ONU os Objetivos de Desenvolvimento do Milênio (ODM), adotados por 191 Estados-membros (incluindo o Brasil), nos quais foram estipulados uma série de compromissos que visavam uma grande melhoria na qualidade de vida da população mundial até 2015. Entre esses objetivos, ficou estabelecido que o Brasil deveria reduzir, até o ano de 2015, a razão de mortalidade materna para 35 óbitos maternos a cada 100.000 nascidos vivos. O país, entretanto, assim como boa parte dos outros países signatários, falhou miseravelmente. Dessa forma, em 2015, foi proposta pela ONU uma nova agenda de desenvolvimento sustentável para os próximos 15 anos, a Agenda 2030, formada por 17 Objetivos de Desenvolvimento Sustentável (ODS). Nesses novos objetivos, o Brasil se comprometeu a reduzir, até 2030, a razão de mortalidade materna para 30 óbitos maternos a cada 100.000 nascidos vivos. Assim, nesse exemplo, utilizaremos um mapa coroplético para analisar o cenário atual desse indicador no país, discutindo sobre os impasses que dificultam a plena realização dos compromissos firmados perante à ONU em tempo hábil.\nPara começo de conversa, um mapa coroplético é um tipo de mapa que utiliza cores de diferentes intensidades para exibir como o valor de uma certa variável varia numa determinada localização geográfica, utilizando colorações mais claras quando o valor dessa variável é menor e colorações mais escuras quando é maior. No nosso caso, estamos interessados em fazer um mapa cloroplético que represente a variação da razão de mortalidade materna de 2019 entre cada estado brasileiro.\nO primeiro passo para a construção desse mapa é obter um arquivo do tipo shapefile que contenha o mapa do Brasil com as delimitações dos estados. Por sorte, o IBGE disponibiliza em seu site tais arquivos para que qualquer pessoa possa realizar os seus downloads. Para facilitar o processo, faremos o download e a descompactação do arquivo de interesse diretamente com o R.\nPrimeiramente, guardaremos no vetor link_ufs o endereço do site do IBGE no qual o arquivo desejado está localizado. Com a função tempdir, do pacote básico base, criaremos um diretório temporário no qual o arquivo será salvo, realizando seu download através da função download.file, do pacote básico utils. Utilizaremos dois argumentos dessa função: o url, que recebe o link de download para o arquivo, e o destfile, que recebe o diretório no qual o arquivo será salvo, bem como seu nome. A descompactação do arquivo será realizada com a função unzip, também do pacote básico utils, na qual utilizamos o argumento zipfile para apontar o diretório no qual o arquivo está localizado, juntamente com seu nome, e o argumento exdir para dizer em qual diretório queremos que o arquivo descompactado se localize. Feito isso, aplicaremos a função st_read, do pacote sf, carregado no início do post, para ler o arquivo BR_UF_2020.shp, salvo no diretório temporário que criamos, e que contém os dados necessários para a construção do mapa brasileiro com as delimitações de cada estado. O resultado desse processo é um data.frame que será guardado no objeto br_ufs.\n\n# Fazendo o download e a descompactação dos arquivos necessários para o mapa coroplético\nlink_ufs &lt;- \"ftp://geoftp.ibge.gov.br/organizacao_do_territorio/malhas_territoriais/malhas_municipais/municipio_2020/Brasil/BR/BR_UF_2020.zip\"\ntmp_dir &lt;- tempdir()\ndownload.file(url = link_ufs, destfile = file.path(tmp_dir, 'BR_UF_2020.zip'))\nunzip(zipfile = file.path(tmp_dir, 'BR_UF_2020.zip'),\n      exdir = tmp_dir)\n\nbr_ufs &lt;- st_read(dsn = file.path(tmp_dir, \"BR_UF_2020.shp\"),\n                  stringsAsFactors = FALSE,\n                  quiet = TRUE)\n\nVeja abaixo as primeiras linhas de br_ufs. Note que as linhas estão ordenadas de acordo com o código dos estados, assim como a tibble dados_maternos com a qual estamos trabalhando.\n\nhead(br_ufs)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -13.6937 xmax: -46.06142 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\n  CD_UF    NM_UF SIGLA_UF NM_REGIAO                       geometry\n1    11 Rondônia       RO     Norte MULTIPOLYGON (((-65.3815 -1...\n2    12     Acre       AC     Norte MULTIPOLYGON (((-71.07772 -...\n3    13 Amazonas       AM     Norte MULTIPOLYGON (((-69.83766 -...\n4    14  Roraima       RR     Norte MULTIPOLYGON (((-63.96008 2...\n5    15     Pará       PA     Norte MULTIPOLYGON (((-51.43248 -...\n6    16    Amapá       AP     Norte MULTIPOLYGON (((-50.45011 2...\n\n\nO próximo passo é adicionar ao data.frame criado acima a variável quantitativa com a qual queremos preencher nosso mapa, que nesse caso é a razão de mortalidade materna estadual do ano de 2019. Como a tibble dados_maternos, criada no início do post, também está ordenada de acordo com o código dos estados, precisamos apenas guardar no vetor mortalidade_2019 o valor das razões de mortalidade materna estaduais do ano de 2019, utilizando para isso a função which, do pacote básico base. Após esse passo, utilizaremos a função mutate para adicionar ao data.frame br_ufs uma nova coluna, nomeada por mort_ufs, que será preenchida pelo vetor mortalidade_2019. Observe, entretanto, que estamos aplicando nesse vetor a função cut, que irá dividi-lo nos intervalos especificados. Assim, os valores que realmente serão atribuídos à coluna mort_ufs serão os intervalos nos quais as razões de mortalidade se encontram. É importante frizar que a concatenação entre o vetor mortalidade_2019 e o data.frame br_ufs só poderá ser feita de forma direta porque ambos estão ordenados de acordo com o código das unidades federativas brasileiras.\n\n# Obtendo as razões de 2019, separando-as em intervalos e adicionando ao data.frame br_ufs\nmortalidade_2019 &lt;- dados_maternos$mortalidade[which(dados_maternos$ano == 2019)]\nbr_ufs &lt;- br_ufs |&gt; mutate(\"mort_ufs\" = cut(mortalidade_2019, c(0, 30, 50, 70, 90)))\n\nO data.frame resultante do processo é o seguinte:\n\nhead(br_ufs)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -13.6937 xmax: -46.06142 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\n  CD_UF    NM_UF SIGLA_UF NM_REGIAO                       geometry mort_ufs\n1    11 Rondônia       RO     Norte MULTIPOLYGON (((-65.3815 -1...  (30,50]\n2    12     Acre       AC     Norte MULTIPOLYGON (((-71.07772 -...  (30,50]\n3    13 Amazonas       AM     Norte MULTIPOLYGON (((-69.83766 -...  (70,90]\n4    14  Roraima       RR     Norte MULTIPOLYGON (((-63.96008 2...  (70,90]\n5    15     Pará       PA     Norte MULTIPOLYGON (((-51.43248 -...  (70,90]\n6    16    Amapá       AP     Norte MULTIPOLYGON (((-50.45011 2...  (30,50]\n\n\nCom os dados preparados, podemos, finalmente, partir para a construção do nosso mapa cloroplético, utilizando para isso a função geom_sf, do pacote ggplot2. O aesthetic fill = mort_ufs, da função ggplot, preencherá os estados de acordo com o intervalo no qual suas razões de mortalidade materna se encontram. Os demais complementos do gráfico já foram explicados ao longo dos exemplos anteriores.\n\n# Construindo o mapa coroplético\nggplot(data = br_ufs, mapping = aes(fill = mort_ufs)) +\n  geom_sf() +\n  labs(title = \"Mapa coroplético da razão de mortalidade materna por estado em 2019\",\n       caption = \"Fontes: IBGE, SIM e SINASC\",\n       fill = paste(c(\"Óbitos maternos a cada\", \"100.000 nascidos vivos\"), collapse = \"\\n\")) +\n  scale_fill_brewer(palette = \"Blues\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\nSe o Brasil planeja honrar o compromisso de reduzir a razão de mortalidade materna para 30 óbitos maternos a cada 100.000 nascidos vivos, claramente há um caminho árduo a ser percorrido. Observando o gráfico acima, podemos perceber que, de todos as 27 unidades federativas brasileiras, apenas o Distrito Federal estava abaixo do limite desejado em 2019, com 21,22 óbitos maternos a cada 100.000 nascidos vivos, enquanto outros 10 estados tinham sua razão de mortalidade materna dentro do intervalo de 30 a 50 óbitos maternos. Sete estados possuíam essa razão entre 70 e 90, dos quais seis se localizam nas regiões Norte ou Nordeste, o que concorda com a discussão anterior sobre as desigualdades regionais existentes no território brasileiro. Vale destacar ainda que estamos considerando os dados de 2019 como os mais atuais, visto que os dados de 2020 e 2021 são preliminares. Esses dois anos, entretanto, foram os anos em que, respectivamente, a pandemia da Covid-19 se iniciou e atingiu seu ápice. Quando consideramos que a pandemia da Influenza A foi responsável por um aumento significativo da razão de mortalidade materna nacional em 2009, mesmo que tenha causado “apenas” 2.101 óbitos totais no país, é assustador imaginar o efeito da atual pandemia nesse indicador, dado que já foram registrados mais de 627 mil óbitos pelo novo Coronavírus desde março de 2020, de acordo com o Ministério da Saúde.\nCaso seja de seu interesse realizar uma leitura mais profunda sobre os dados do gráfico, terminaremos este exemplo construindo uma tabela que contém as razões de mortalidade materna estaduais em sua forma bruta, sem a divisão por intervalos. Para isso, filtraremos, primeiramente, a tibble dados_maternos para obter somente as linhas cujo ano é 2019, utilizando nesse processo a função filter. Na tibble resultante, aplicaremos a função select para selecionar todas as variáveis que queremos que estejam na tabela, e que nesse caso serão a sigla da unidade federativa, a região à qual pertence, o número de nascidos vivos e de óbitos maternos em 2019, e a razão de mortalidade materna de cada estado nesse mesmo período. Com a função arrange, ordenaremos os dados de forma crescente em relação à variável mortalidade. Por fim, com rename, renomearemos as colunas com os nomes que devem constar na tabela. A tibble resultante desse passo será guardada no objeto dados_2019, e todas as funções utilizadas nessa sequência pertencem ao pacote dplyr.\n\n# Preparando os dados para a construção da tabela\ndados_2019 &lt;- filter(dados_maternos, ano == 2019) |&gt;\n  select(uf, regiao, nascimentos, obitos, mortalidade) |&gt;\n  arrange(mortalidade) |&gt;\n  rename(\"UF\" = uf, \n         \"Região\" = regiao, \n         \"Nº de nascimentos\" = nascimentos,\n         \"Nº de óbitos maternos\" = obitos, \n         \"Razão de mortalidade materna\" = mortalidade)\n\nPara a construção da tabela, será utilizada a função datatable, do pacote DT, carregado no início do post. O primeiro argumento da função, data, recebe a tibble dados_2019, criada no passo anterior. Em caption, diremos qual deve ser a legenda da tabela. O argumento options recebe uma lista de customizações que a tabela deve conter: desabilitamos a caixa de pesquisa utilizando searching = FALSE; com paging = FALSE, impedimos que a tabela seja separada em páginas; por fim, através de columnDefs = list(list(className = “dt-center”, targets = “_all”))), determinamos que o alinhamento de todas as colunas deve ser centralizado. O resultado final de todo o processo pode ser visto a seguir.\n\n# Construindo a tabela\ndatatable(data = dados_2019, \n          caption = \"Tabela: Razão de mortalidade materna das unidades federativas brasileiras em 2019\",\n          rownames = FALSE,\n          options = list(searching = FALSE,\n                         paging = FALSE,\n                         columnDefs = list(list(className = \"dt-center\", targets = \"_all\")))\n          )\n\n\n\n\n\n\nE com isso, aqui chegamos ao fim desse post. Esperamos que, por meio dessa leitura, os conceitos relativos à razão de mortalidade materna tenham ficado claros, e que os exemplos apresentados ao longo do texto tenham sido informativos o suficiente ao ponto de ser possível não só entender melhor sobre esse indicador, mas também descobrir novas funções e tipos diferentes de análises, principalmente visuais, que podem ser feitas através do R.\n\n\nReferências\nhttps://bvsms.saude.gov.br/bvs/publicacoes/qualificacao_saude_sup/pdf/Atenc_saude3fase.pdf\nhttp://svs.aids.gov.br/dantps/cgiae/sinasc/\nhttp://svs.aids.gov.br/dantps/cgiae/sim/\nhttps://g1.globo.com/Sites/Especiais/Noticias/0,,MUL1191163-16726,00-ORGANIZACAO+MUNDIAL+DE+SAUDE+DECLARA+PANDEMIA+DA+NOVA+GRIPE.html\nhttps://piaui.folha.uol.com.br/lupa/2020/03/27/verificamos-coronavirus-h1n1/\nhttps://portaldeboaspraticas.iff.fiocruz.br/biblioteca/desenvolvimento-humano-nas-macrorregioesbrasileiras/\nhttps://www.elsevier.es/es-revista-reproducao-climaterio-385-articulo-mortalidade-materna-no-brasil-insucesso-S1413208714000089\nhttps://paulofelipe.github.io/bag_of_posts/posts/2018-10-21-mapa-coropltico-com-ggplot2/\nhttps://covid.saude.gov.br/\nhttps://www.ibge.gov.br/geociencias/organizacao-do-territorio/malhas-territoriais/15774-malhas.html?=&t=downloads\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCitationBibTeX citation:@online{sant'ana herzog2022,\n  author = {Sant’Ana Herzog, Rafael},\n  title = {Entendendo, Calculando e Visualizando a Razão de Mortalidade\n    Materna Do {Brasil} Com o {R}},\n  date = {2022-01-25},\n  url = {https://rafaherzog.github.io/rafasantanah/posts/25-01-2022-rmm/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSant’Ana Herzog, Rafael. 2022. “Entendendo, Calculando e\nVisualizando a Razão de Mortalidade Materna Do Brasil Com o R.”\nJanuary 25, 2022. https://rafaherzog.github.io/rafasantanah/posts/25-01-2022-rmm/."
  },
  {
    "objectID": "posts/28-06-2022-intervalos-de-confiança/index.html",
    "href": "posts/28-06-2022-intervalos-de-confiança/index.html",
    "title": "Intervalos de confiança",
    "section": "",
    "text": "Em situações da vida real, é comum encontrarmos estimativas para quantidades desconhecidas acompanhadas de uma certa margem de erro. Quando estamos em épocas próximas de eleições, por exemplo, ouvimos a todo momento que um certo candidato tem, digamos, 40% das intenções de voto, com uma margem de erro de 2 pontos percentuais para mais, e 2 pontos percentuais para menos. Em outras palavras, estamos confiantes de que o verdadeiro valor das intenções de voto para esse candidato se encontra entre 38% e 42%. Estimativas pontuais, por mais úteis que sejam, acabam fornecendo uma informação incompleta sobre o valor estimado do parâmetro em questão, uma vez que elas não levam em consideração a variância do estimador utilizado para se obter tal estimativa. Dessa forma, como no exemplo da pesquisa eleitoral, é desejável que uma estimativa pontual seja acompanhada de alguma medida de seu possível erro, podendo essa medida ser, por exemplo, um intervalo relacionado a uma dimensão da confiança que temos de o verdadeiro valor do parâmetro estar sendo captado. Assim, nesse post, conheceremos sobre um dos tipos de estimativas intervalares mais utilizados dentro da Estatística: os intervalos de confiança. Apresentaremos a sua definição, discutiremos sobre como interpretá-los corretamente e estudaremos sobre um dos métodos existentes para obtê-los, juntamente com uma série de exemplos que permitam absorver melhor o conteúdo discutido.\nAntes de mais nada, é preciso que dois conceitos importantíssimos estejam bem claros. Em primeiro lugar, considere uma certa distribuição de probabilidade f. Dizemos que n variáveis aleatórias X1, …, Xn formam uma amostra aleatória dessa distribuição se todas as variáveis aleatórias forem independentes e se a distribuição marginal de cada variável for f. Em outras palavras, para que X1, …, Xn formem uma amostra aleatória, elas devem ser variáveis aleatórias independentes e identicamente distribuídas (i.i.d.). A segunda definição é em relação ao termo estatística: chamamos de estatística qualquer função de variáveis aleatórias observáveis que não dependa de quantidades desconhecidas, sendo ela própria uma variável aleatória observável.\nCom esses conceitos em mente, passemos, sem mais enrolação, para a definição de intervalo de confiança. Seja (X1, …, Xn) uma amostra aleatória de uma distribuição que dependa de um parâmetro (ou vetor de parâmetros) \\(\\theta\\). Seja g(\\(\\theta\\)) uma função real de \\(\\theta\\), e sejam T1 = t1(X1, …, Xn) e T2 = t2(X1, …, Xn) duas estatísticas tais que T1 \\(\\leqslant\\) T2 e que possuem a propriedade de que, para todos os valores de \\(\\theta\\),\n\\[\nP(T_1 &lt; g(\\theta) &lt; T_2) \\geqslant 1 - \\alpha,\n\\] com \\(0 &lt; \\alpha &lt; 1\\). Então, o intervalo aleatório (T1, T2) é chamado de intervalo de 100(1 - \\(\\alpha\\))% de confiança para g(\\(\\theta\\)). Além disso, temos que\n\n\\(\\gamma = 1 - \\alpha\\) é chamado de coeficiente de confiança;\nT1 e T2 são chamados, respectivamente, de limites de confiança inferior e superior para g(\\(\\theta\\)).\n\nSe a inequação “\\(\\geqslant 1 - \\alpha\\)” se resumir a uma igualdade para todos os valores possíveis de \\(\\theta\\), dizemos que o intervalo de confiança é exato (para distribuições contínuas, o intervalo será sempre exato; a desigualdade é útil para quando estamos trabalhando com distribuições discretas, para as quais é comum não ser possível satisfazer a igualdade). Quando observamos, de fato, os valores de X1, …, Xn por meio da amostra e calculamos os valores de T1 = t1 e T2 = t2, obtemos o intervalo (t1, t2), que é chamado de valor observado do intervalo de confiança (apesar de, na prática, também dizermos que (t1, t2) é simplesmente um intervalo de 100(1 - \\(\\alpha\\))% de confiança para \\(\\theta\\)).\nA interpretação de um intervalo de confiança deve ser feita com cuidado. É preciso entender que o intervalo aleatório (T1, T2) possui probabilidade (1 - \\(\\alpha\\)) de conter o verdadeiro valor do parâmetro que estamos estimando, afirmação essa que não pode ser feita para o intervalo de confiança observado (t1, t2). Isso porque, antes de colhermos a amostra, as estatísticas T1 e T2 são variáveis aleatórias e, portanto, podemos encontrar a probabilidade de \\(\\theta\\) estar dentro do intervalo formado por elas. A partir do momento em que colhemos a amostra e observamos os valores t1 e t2 das estatísticas, entretanto, o intervalo simplesmente conterá, ou não, o valor real do parâmetro; não é mais uma questão de probabilidade. Assim, podemos dizer apenas que temos uma confiança considerável de que o intervalo observado contém o verdadeiro valor do parâmetro. A medida da nossa confiança é de (1 - \\(\\alpha\\)) porque, antes de colhermos a amostra, (1 - \\(\\alpha\\)) era a probabilidade de que o intervalo aleatório contivesse o verdadeiro valor de \\(\\theta\\).\nUma outra possível interpretação do conceito de intervalo de confiança é a seguinte: se obtivéssemos várias amostras de mesmo tamanho e, para cada uma delas, calculássemos os correspondentes intervalos de confiança com coeficiente de confiança (1 - \\(\\alpha\\)), esperaríamos que a proporção de intervalos que contivessem o verdadeiro valor do parâmetro fosse igual a (1 - \\(\\alpha\\)).\nPor fim, podemos fazer algumas considerações a respeito da escolha do valor de \\(\\gamma\\) (lembrando que \\(\\gamma = 1 - \\alpha\\)). Como veremos nos exemplos subsequentes, conforme aumentamos o coeficiente de confiança, os valores de T1 e T2 ficam maiores e, consequentemente, a amplitude do intervalo aumenta (sendo a amplitude definida como a diferença entre os extremos superior e inferior de um intervalo). Isso, claro, é algo que deveríamos esperar, visto que intervalos maiores possuem naturalmente uma maior chance de conterem o verdadeiro valor de um parâmetro desconhecido. Com isso, para que os intervalos sejam o mais informativo possível, mantendo uma confiança elevada, é necessário que selecionemos \\(\\gamma\\) de forma balanceada, sendo uma escolha muito comum o valor 0,95 (nesse caso, temos que \\(\\alpha\\) = 0,05). É muito mais interessante, por exemplo, um intervalo de confiança que diz que o verdadeiro valor do salário médio de um estatístico está entre 3,5 a 6 salários mínimos do que um intervalo que diz que esse valor está entre 2 a 7,5 salários mínimos. Apesar de, com o segundo intervalo, termos uma maior confiança de que o verdeiro valor do salário médio está sendo captado, a qualidade da informação que extraímos dele é consideravelmente pior do que aquela obtida com o primeiro intervalo.\nCom esses conceitos iniciais em mente, passemos, agora, para exemplos da construção de intervalos de confiança em algumas situações mais usuais, utilizando para isso um método denominado método da quantidade pivotal."
  },
  {
    "objectID": "posts/28-06-2022-intervalos-de-confiança/index.html#intervalo-de-confiança-para-a-média---com-variância-conhecida",
    "href": "posts/28-06-2022-intervalos-de-confiança/index.html#intervalo-de-confiança-para-a-média---com-variância-conhecida",
    "title": "Intervalos de confiança",
    "section": "Intervalo de confiança para a média - com variância conhecida",
    "text": "Intervalo de confiança para a média - com variância conhecida\nComeçando do exemplo mais simples, considere uma amostra aleatória (X1, …, Xn) da distribuição N(\\(\\mu\\), \\(\\sigma\\)^2), com variância populacional conhecida, e suponha que queremos criar um intervalo de confiança para \\(\\mu\\). Utilizando o método apresentado acima, o primeiro passo para a resolução desse problema é encontrar uma função que dependa apenas da amostra e de \\(\\mu\\), e cuja distribuição de probabilidade independa de tal parâmetro, para que possamos utilizar como quantidade pivotal. Note, primeiramente, que, sendo X1,…, Xn variáveis normais, independentes e identicamente distribuídas, sabemos que\n\\[\n\\bar{X} \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right),\n\\] sendo\n\\[\n\\bar{X} = \\frac{\\sum_i^n X_i}{n}.\n\\]\nDessa forma, subtraindo a média e dividindo o resultado pelo desvio padrão, podemos padronizar a variável acima, a saber\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\]\nPerceba que, como \\(\\sigma\\) é conhecido, nada impede que esse termo esteja presente na quantidade pivotal que queremos encontrar. Logo, como Z é uma função da amostra e de \\(\\mu\\), e como sua distribuição independe de tal parâmetro, podemos utilizá-la como quantidade pivotal para a criação do intervalo de confiança desejado. Assim, definindo um valor de \\(\\alpha\\), com 0 &lt; \\(\\alpha\\) &lt; 1, podemos encontrar z1 e z2 dependentes de \\(\\alpha\\) tais que\n\\[\nP\\left(z_1 &lt; Z &lt; z_2\\right) = 1 - \\alpha.\n\\]\nSubstituindo Z pela expressão obtida acima,\n\\[\nP\\left(z_1 &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} &lt; z_2 \\right) = 1 - \\alpha.\n\\]\nIsolando \\(\\mu\\),\n\\[\n\\begin{align}\nP\\left(z_1 &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} &lt; z_2 \\right) & = P\\left(z_1\\sigma &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; z_2\\sigma \\right) \\\\\n& = P\\left(z_1\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; z_2\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\\n& = P\\left(-\\bar{X} + z_1\\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + z_2\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\\n& = P\\left(\\bar{X} - z_2\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_1\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha \\qquad \\text{(I)}\n\\end{align}\n\\]\nNote, entretanto, que ainda precisamos definir quem serão z1 e z2, uma vez que existem infinitos pares de valores que satisfazem P(z1 &lt; Z &lt; z2) = 1 - \\(\\alpha\\). Como é de nosso interesse obter o intervalo que ofereça a melhor informação possível, é necessário que encontremos z1 e z2 que minimizem a amplitude do intervalo de confiança, que nesse caso é dada por\n\\[\n\\bar{X} - z_1\\frac{\\sigma}{\\sqrt{n}} - \\left(\\bar{X} - z_2\\frac{\\sigma}{\\sqrt{n}}\\right) = \\frac{\\sigma}{\\sqrt{n}}(z_2 - z_1).\n\\]\nDessa forma, o intervalo de valores plausíveis do parâmetro desconhecido será o menor possível, e, com isso, podemos ter uma noção mais precisa do verdadeiro valor do parâmetro que estamos estimando. Para distribuições de probabilidade simétricas, obter essas quantidades é simples: definido um valor de \\(\\alpha\\), e considerando uma quantidade pivotal W cuja distribuição é simétrica, basta que encontremos w1 e w2 tais que\n\\[\nP(W \\leqslant w_1) = P(W &gt; w_2) = \\frac{\\alpha}{2},\n\\] e denotamos esses valores por\n\\[\nw_1 = w_{\\alpha/2} \\text{ e } w_2 = w_{1 - \\alpha/2}.\n\\]\nAlém disso, caso a distribuição de probabilidade da variável seja simétrica em torno de zero, é possível mostrar que a amplitude do intervalo será mínima se w1 e w2 forem valores opostos, ou seja, se w1 = -w2.\nVoltando para o exemplo em questão, como a quantidade pivotal Z segue distribuição normal padrão, que é uma distribuição simétrica em torno de zero, precisamos apenas encontrar z tal que\n\\[\nP(Z \\leqslant z) = 1 - \\frac{\\alpha}{2}.\n\\]\nDenotaremos esse valor de z por\n\\[\nz_{1 -\\alpha/2},\n\\]\ne utilizaremos notações semelhantes ao longo das explicações. Com isso, podemos reescrever z1 e z2 como sendo\n\\[\nz_1 = -z_{1 - \\alpha/2} \\text{ e } z_2 = z_{1 - \\alpha/2}.\n\\] Para absorver melhor a explicação acima, observe o gráfico a seguir, que representa a curva da densidade de probabilidade da distribuição normal padrão.\n\n\n\n\n\nFigura 1: Para uma confiança de (100 - \\(\\alpha\\))%, a área em cada cauda da distribuição deverá ser de \\(\\alpha\\)/2 para que o intervalo seja o menor possível.\n\n\n\n\nEm uma situação prática, na qual teríamos um valor definido de \\(\\alpha\\), poderíamos utilizar uma tabela da distribuição normal padrão para encontrar o valor z de interesse, ou mesmo utilizar a função qnorm(), do pacote básico {stats}, do R, para realizar esse processo. A função qnorm(), bem como a família de funções do R que seguem a estrutura “qnome_da_distribuição()”, representa a função quantílica: para uma dada probabilidade e para dados valores dos parâmetros da distribuição, a função retorna o valor para o qual a probabilidade acumulada até o ponto seja a probabilidade estipulada em seus argumentos.\nVoltando, por fim, à equação I, temos\n\\[\nP\\left(\\bar{X} - z_2\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_1\\frac{\\sigma}{\\sqrt{n}} \\right) = P\\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha.\n\\]\nPortanto, um intervalo de 100(1 - \\(\\alpha\\))% de confiança para \\(\\mu\\), quando a variância populacional é conhecida, é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\; \\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nSubstituindo as letras por números, considere o seguinte exemplo. Suponha que, por analogia a produtos similares, o tempo de reação de um novo medicamento possa ser considerado como tendo uma distribuição Normal com média \\(\\mu\\), desconhecida, e desvio padrão igual a 2 minutos. Considere que uma amostra aleatória de vinte e cinco pacientes foi obtida, que cada paciente recebeu o medicamento e que o tempo médio de reação do medicamento nesses pacientes tenha sido de 4,5 minutos. A pedido de um farmacêutico, queremos verificar se 6 minutos é um tempo plausível para o verdadeiro valor do tempo médio de reação do medicamento. Dessa forma, podemos resolver esse problema a partir da criação de um intervalo de confiança para \\(\\mu\\), que é o valor populacional do tempo médio de reação. Do enunciado, sabemos que:\n\nPodemos utilizar o intervalo de confiança obtido acima para o problema em questão, uma vez que os dados seguem distribuição normal com média desconhecida e desvio padrão conhecido e igual a 2;\nO tamanho da amostra, n, é 25;\nA média amostral é 4,5.\n\nAssim, adotando um nível de confiança de 95% (logo, \\(\\alpha\\) = 0,05), e substituindo os valores conhecidos na fórmula acima, temos:\n\\[\n\\begin{align}\nIC(\\mu,\\ 95\\%) & = \\left(4,5 - z_{1 - 0,05/2}\\left(\\frac{2}{\\sqrt{25}}\\right);\\; 4,5 + z_{1 - 0,05/2}\\left(\\frac{2}{\\sqrt{25}}\\right)\\right) \\\\\n& = \\left(4,5 - z_{1 - 0,025}\\left(\\frac{2}{5}\\right);\\; 4,5 + z_{1 - 0,025}\\left(\\frac{2}{5}\\right)\\right) \\\\\n& = \\left(4,5 - z_{0,975}(0,4);\\; 4,5 + z_{0,975}(0,4)\\right)\n\\end{align}\n\\]\nUtilizando a função qnorm(), já explicada anteriormente, para encontrar o quantil da normal padrão tal que, dele para trás, a probabilidade acumulada seja de 0,975,\n\nqnorm(p = 0.975, mean = 0, sd = 1)\n\n[1] 1.959964\n\n\nAproximando o resultado acima, temos que\n\\[\nz_{1 - \\alpha/2} = z_{0,975} = 1,96.\n\\]\nAssim, voltando ao intervalo de confiança,\n\\[\n\\begin{align}\nIC(\\mu,\\ 95\\%) & = \\left(4,5 - 1,96(0,4);\\ 4,5 + 1,96(0,4)\\right) \\\\\n& = (4,5 - 0,784;\\ 4,5 + 0,784) \\\\\n& = (3,716;\\ 5,284).\n\\end{align}\n\\]\nPortanto, um intervalo de 95% de confiança para o tempo médio de reação do medicamento é de (3,716; 5,284). Dessa forma, podemos concluir, com 95% de confiança, que 6 não é um valor plausível para o verdadeiro valor do parâmetro desconhecido \\(\\mu\\).\nAlgumas considerações importantes que podem ser feitas aqui são o efeito da escolha de \\(\\alpha\\) e do valor de n na amplitude do intervalo encontrado. Se tivéssemos escolhido como nível de confiança o valor de 99%, teríamos como resultado o intervalo (3,47; 5,53). Observe que a amplitude desse intervalo é de 2,06, enquanto a amplitude do primeiro é de 1,568. Dessa forma, a qualidade da informação que obtemos com o intervalo de 95% de confiança é claramente melhor do que a obtida utilizando uma confiança maior. Considere agora que o tamanho da amostra aleatória coletada seja n = 100, e que a média amostral se manteve em 4,5 minutos. Um intervalo de 95% de confiança para essa situação seria (4,108; 4,892), que é significativamente mais informativo do que os intervalos obtidos quando o tamanho da amostra era 25. Dessa forma, podemos perceber, como é intuitivamente esperado, que é de forte interesse obter amostras que sejam as maiores possíveis, visto que as estimativas obtidas por meio delas são consideravelmente mais precisas.\n\nNo R\nComo não vivemos mais em tempos arcaicos, nos quais uma folha de papel, um lápis e tabelas de valores críticos eram os únicos artifícios disponíveis, podemos, é claro, utilizar o R para facilitar a criação de intervalos de confiança. Aproveitando o exemplo acima, suponha que os tempos de reação do medicamento observados na amostra de 25 pacientes tenham sido os seguintes:\n\namostra &lt;- c(1.7, 2.8, 3.3, 6.4, 3.0, 2.0, 9.3, 3.7, 4.8, 3.0, 5.8, 2.8, 4.7, 6.1, 2.7, 4.2, 2.6, 4.6,\n           4.5, 7.0, 0.2, 7.0, 6.5, 7.7, 6.1)\n\nNesse passo, criaremos uma função, a qual chamaremos de ic_media_caso1(), que calculará intervalos de confiança para o parâmetro \\(\\mu\\), da distribuição normal, quando a variância populacional é conhecida. A função possuirá três argumentos: “dados”, que receberá o vetor de valores observados na amostra; “sigma”, que receberá o valor da variância populacional; e “gamma”, que receberá o coeficiente de confiança associado ao intervalo. Calcularemos a média e o tamanho da amostra utilizando, respectivamente, as funções mean() e length(), ambas do pacote básico {base}, e utilizaremos a já explicada função qnorm() para encontrar o valor do quantil da normal padrão (não utilizaremos os argumentos “mean” e “sigma” dessa função, uma vez que seus valores padrão são, respectivamente, 1 e 0). Construiremos o intervalo nos vetores “limite_inferior” e “limite_superior”, utilizando a formulação obtida mais acima. A função retornará um data.frame contendo algumas medidas referentes à amostra e o intervalo de confiança propriamente dito. O resultado do processo pode ser visto abaixo.\n\nic_media_caso1 &lt;- function(dados, sigma, gamma) {\n  media &lt;- mean(dados)\n  n &lt;- length(dados)\n  alfa &lt;- 1 - gamma\n  z &lt;- round(qnorm(p = 1 - alfa/2), 3)\n  limite_inferior &lt;- round(media - z * sigma/sqrt(n), 3)\n  limite_superior &lt;- round(media + z * sigma/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(data.frame(n, media, limite_inferior, limite_superior, amplitude))\n}\n\nic_media_caso1(dados = amostra, sigma = 2, gamma = 0.95)\n\n   n media limite_inferior limite_superior amplitude\n1 25   4.5           3.716           5.284     1.568\n\n\nComo podemos perceber, o resultado obtido, como já era de se esperar, foi o mesmo que o encontrado anteriormente. Apesar de um pouquinho cansativo, este primeiro exemplo servirá de base para o que está por vir. Vamos ao próximo."
  },
  {
    "objectID": "posts/28-06-2022-intervalos-de-confiança/index.html#intervalo-de-confiança-para-a-média---com-variância-desconhecida",
    "href": "posts/28-06-2022-intervalos-de-confiança/index.html#intervalo-de-confiança-para-a-média---com-variância-desconhecida",
    "title": "Intervalos de confiança",
    "section": "Intervalo de confiança para a média - com variância desconhecida",
    "text": "Intervalo de confiança para a média - com variância desconhecida\nNessa segunda situação, considere agora uma amostra aleatória (X1, …, Xn) da distribuição N(\\(\\mu\\), \\(\\sigma\\)^2), com variância populacional desconhecida, e suponha que queremos novamente criar um intervalo de confiança para \\(\\mu\\). Assim como no exemplo anterior (e em todos os que estão por vir), o primeiro passo é encontrarmos uma quantidade pivotal que nos permita construir tal intervalo. Como definido anteriormente, sabemos que\n\\[\nZ = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\]\nLembre-se, entretanto, que estamos no caso em que \\(\\sigma\\) é desconhecido, e por isso não podemos utilizar a função acima como quantidade pivotal nesse problema. Assim, utilizaremos dois resultados conhecidos dentro da Estatística para chegar em uma função que nos seja útil. Em primeiro lugar, se (X1, …, Xn) formam uma amostra aleatória da distribuição N(\\(\\mu\\), \\(\\sigma\\)^2), temos que\n\\[\nQ = \\frac{\\left(n - 1\\right) S^2}{\\sigma^2} \\sim \\chi^2_{n-1},\n\\]\nonde\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})}{n -1}\n\\] é a variância amostral. Além disso, é possível demonstrar que Z e Q são independentes. O outro resultado que utilizaremos para encontrar a quantidade pivotal é o seguinte: se temos duas variáveis aleatórias independentes U e W tais que\n\\[\nU \\sim N(0, 1)\\text{ e } W \\sim \\chi^2_v,\n\\]\nentão é válido que\n\\[\nT = \\frac{U}{\\sqrt{W/v}} \\sim t_{v},\n\\]\nonde \\(t_{v}\\) representa a distribuição T de Student com v graus de liberdade. Convenientemente, definimos acima variáveis independentes Z e Q que seguem, respectivamente, distribuição normal padrão e distribuição qui-quadrado com (n - 1) graus de liberdade. Dessa forma, nada nos impede de utilizar o resultado anterior para tentarmos obter alguma função de interesse. Assim, substituindo as devidas expressões, temos\n\\[\n\\begin{align}\nT = \\frac{Z}{\\sqrt{Q/(n-1)}} & = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)/\\sigma}{\\sqrt{\\frac{\\left(n - 1\\right) S^2/\\sigma^2}{n - 1}}} \\\\\n& = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)/\\sigma}{\\sqrt{\\frac{\\left(n - 1\\right) S^2}{\\sigma^2(n - 1)}}} \\\\\n& = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)/\\sigma}{S/\\sigma} \\\\\n& = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} \\sim t_{n - 1}\n\\end{align}\n\\]\nE voilà! Acabamos encontrando uma função que depende apenas da amostra e de \\(\\mu\\), sem depender de \\(\\sigma\\), e cuja distribuição independe de ambos os parâmetros desconhecidos. Logo, podemos utilizá-la como quantidade pivotal para construirmos um intervalo de confiança para \\(\\mu\\). Assim, definido um valor de \\(\\alpha\\), com 0 &lt; \\(\\alpha\\) &lt; 1, podemos encontrar t1 e t2 tais que\n\\[\nP\\left(t_{1_{(n-1)}} &lt; T &lt; t_{2_{(n-1)}} \\right) = 1 - \\alpha.\n\\] Substituindo T pela expressão encontrada acima,\n\\[\nP\\left(t_{1_{(n-1)}} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; t_{2_{(n-1)}} \\right) = 1 - \\alpha.\n\\]\nIsolando \\(\\mu\\),\n\\[\n\\begin{align}\nP\\left(t_{1_{(n-1)}} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; t_{2_{(n-1)}} \\right) & = P\\left(t_{1_{(n-1)}} S &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; t_{2_{(n-1)}} S\\right) \\\\\n& = P\\left(t_{1_{(n-1)}} \\frac{S}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; t_{2_{(n-1)}} \\frac{S}{\\sqrt{n}} \\right) \\\\\n& = P\\left(-\\bar{X} + t_{1_{(n-1)}} \\frac{S}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + t_{2_{(n-1)}} \\frac{S}{\\sqrt{n}} \\right) \\\\\n& = P\\left(\\bar{X} - t_{2_{(n-1)}} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - t_{1_{(n-1)}} \\frac{S}{\\sqrt{n}} \\right) = 1 - \\alpha \\qquad \\textrm{(II)}\n\\end{align}\n\\]\nPara obtermos os valores de t1 e t2, utilizaremos o mesmo procedimento detalhado no exemplo anterior, uma vez que a distribuição T de Student também é uma distribuição simétrica em torno de zero. Assim, basta que encontremos t tal que\n\\[\nP(T \\leqslant t) = 1 - \\frac{\\alpha}{2},\\text{ com }T \\sim t_{n - 1},\n\\]\no qual denotaremos por\n\\[\nt_{(n - 1;\\;1 -\\alpha/2)}.\n\\]\nCom isso, podemos definir t1 e t2 como sendo\n\\[\nt_1 = -t_{(n - 1;\\;1 -\\alpha/2)} \\text{ e } t_2 = t_{(n - 1;\\;1 -\\alpha/2)}.\n\\] Para um melhor entendimento, observe o gráfico abaixo, que representa a curva da densidade de probabilidade da distribuição T de Student com n graus de liberdade.\n\n\n\n\n\nFigura 2: Para uma confiança de (100 - \\(\\alpha\\))%, a área em cada cauda da distribuição deverá ser de \\(\\alpha\\)/2 para que o intervalo seja o menor possível.\n\n\n\n\nNovamente, em uma situação prática, na qual teríamos um valor definido de \\(\\alpha\\), poderíamos utilizar tanto uma tabela da distribuição T de Student quanto a função qt(), do R, para encontrar o valores de t1 e t2. De qualquer forma, retornando à equação II, temos:\n\\[\nP\\left(\\bar{X} - t_{2_{(n-1)}} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - t_{1_{(n-1)}} \\frac{S}{\\sqrt{n}} \\right) = P\\left(\\bar{X} - t_{(n - 1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) = 1 - \\alpha.\n\\]\nPortanto, um intervalo de 100(1 - \\(\\alpha\\))% de confiança para \\(\\mu\\), quando a variância populacional é desconhecida, é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}}; \\bar{X} + t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}} \\right).\n\\]\nAlgo interessante a se observar é que tanto o intervalo aleatório acima quanto o intervalo aleatório encontrado no exemplo anterior possuem 95% de probabilidade de conterem o verdadeiro valor do parâmetro \\(\\mu\\), mesmo que suas estruturas sejam diferentes. Entretanto, como na construção do intervalo apresentado no presente exemplo foi utilizado um estimador de \\(\\sigma\\), a amplitude média dos intervalos de confiança obtidos por esse método será maior do que a amplitude média dos intervalos obtidos quando utilizamos o método apresentado no exemplo anterior. Com isso, dizemos que o intervalo de confiança acima é menos informativo, uma vez que o intervalo de valores plausíveis para \\(\\mu\\), nesse caso, será maior que o do intervalo de confiança construído anteriormente. Esse problema é mais evidente para tamanhos pequenos de amostra, e torna-se menos relevante conforme o valor de n aumenta.\n\nNo R\nPara demonstrar a diferença entre o intervalo de confiança derivado acima e aquele derivado no exemplo anterior, podemos aproveitar o mesmo exemplo sobre o tempo de reação de um medicamento previamente discutido. Novamente, criaremos uma função, a qual chamaremos de ic_media_caso2(), que retornará o intervalo de confiança para a média populacional quando utilizamos uma amostra aleatória da distribuição normal, considerando a variância populacional desconhecida. A função se dará de forma similar àquela criada anteriormente, sendo as únicas diferenças o cálculo do desvio padrão amostral, feito com a função sd(), do pacote básico {stats} e a obtenção do quantil da distribuição T de Student através da função qt(), do mesmo pacote. O resultado do processo pode ser visto abaixo. Lembre-se que o vetor “amostra” foi criado no exemplo anterior.\n\nic_media_caso2 &lt;- function(dados, gamma) {\n  media &lt;- mean(dados)\n  n &lt;- length(dados)\n  S &lt;- round(sd(dados), 3)\n  alfa &lt;- 1 - gamma\n  t &lt;- qt(1 - alfa/2, n - 1)\n  limite_inferior &lt;- round(media - t * S/sqrt(n), 3)\n  limite_superior &lt;- round(media + t * S/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(data.frame(n, media, S, limite_inferior, limite_superior, amplitude))\n}\n\nic_media_caso2(dados = amostra, gamma = 0.95)\n\n   n media     S limite_inferior limite_superior amplitude\n1 25   4.5 2.162           3.608           5.392     1.784\n\n\nComo podemos perceber, o intervalo de confiança obtido acima apresenta uma amplitude levemente maior do que o intervalo obtido anteriormente (1,784 contra 1,568), no qual utilizamos a informação de que a variância populacional era conhecida durante o processo da construção do intervalo. Dessa forma, apesar de a diferença não ser gritante, podemos perceber que a utilização de uma estimativa para o valor da variância populacional acarretou, como já explicado anteriormente, em um intervalo levemente menos informativo a respeito do verdadeiro valor da média populacional.\nCom isso, aqui encerramos os exemplos de intervalos de confiança para a distribuição normal construídos a partir do método da quantidade pivotal. Como considerações finais, podemos dizer que:\n\nÉ possível mostrar que, para qualquer amostra de uma distribuição de probabilidade contínua, pelo menos uma quantidade pivotal existe;\nSe \\(\\theta\\) for um parâmetro de localização (ou seja, um parâmetro que determina onde a distribuição está centrada no eixo x e que, quando alterado, é responsável por transladar o gráfico da função densidade/massa de probabilidade para a esquerda ou para a direita), então a função\n\n\\[\nX_i - \\theta\n\\] possui distribuição independente de \\(\\theta\\) e, portanto, pode ser utilizada como uma quantidade pivotal;\n\nSe \\(\\theta\\) for um parâmetro de escala (ou seja, um parâmetro que, quando alterado, é responsável por expandir ou comprimir o gráfico da função densidade/massa de probabilidade da distribuição), então a função\n\n\\[\n\\frac{X_i}{\\theta}\n\\] possui distribuição independente de \\(\\theta\\) e, portanto, pode ser utilizada como uma quantidade pivotal."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "O valor-p\n\n\n\n\n\n\n\nestatística\n\n\nR\n\n\n\n\nEntendendo o conceito, a interpretação e a maneira correta de se utilizar o valor-p em testes de hipóteses\n\n\n\n\n\n\nAug 24, 2023\n\n\nRafael Sant’Ana Herzog\n\n\n\n\n\n\n  \n\n\n\n\nFunção de verossimilhança\n\n\n\n\n\n\n\nestatística\n\n\nR\n\n\n\n\nEntendendo o conceito e exemplificando a utilização da função de verossimilhança dentro da Estatística\n\n\n\n\n\n\nMar 15, 2022\n\n\nRafael Sant’Ana Herzog\n\n\n\n\n\n\n  \n\n\n\n\nIntervalos de confiança\n\n\n\n\n\n\n\nestatística\n\n\nR\n\n\n\n\nComo construir, aplicar e interpretar essas ferramentas estatísticas\n\n\n\n\n\n\nMar 15, 2022\n\n\nRafael Sant’Ana Herzog\n\n\n\n\n\n\n  \n\n\n\n\nEntendendo, calculando e visualizando a razão de mortalidade materna do Brasil com o R\n\n\n\n\n\n\n\nestatística\n\n\nR\n\n\nsaúde\n\n\n\n\nExplorando e realizando algumas discussões sobre esse indicador dentro do país por meio do R\n\n\n\n\n\n\nJan 25, 2022\n\n\nRafael Sant’Ana Herzog\n\n\n\n\n\n\nNo matching items"
  }
]